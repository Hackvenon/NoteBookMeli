{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bed36ae",
   "metadata": {},
   "source": [
    "\n",
    "# INFORME TECNICO DETALLADO - EXPLOTACION IDOR - MELI\n",
    "# Prueba Tecnica Meli - 2025\n",
    "# Andres Mateo Diaz Ca√±on\n",
    "\n",
    "El presente informe documenta el an√°lisis realizado sobre los registros de red proporcionados en el archivo ‚Äúthree_months_logs.csv‚Äù. El objetivo fue determinar la existencia de actividad an√≥mala vinculada a una vulnerabilidad de tipo Insecure Direct Object Reference (IDOR) en el endpoint /invoices/search de www.mercadolibre.com.\n",
    "\n",
    "> **Importante:** Cambia `CSV_FILE` si tu ruta es distinta, y ajusta `delimiter` si tu CSV usa `;`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65994f9",
   "metadata": {},
   "source": [
    "## 1) Requisitos y  Verificacion de complementos\n",
    "\n",
    "Para el funcionamiento correcto del Notebook, se requiere la instalacion de los siguientes paquetes y librerias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Requisitos b√°sicos\n",
    "!python -m pip install langchain duckdb openai llama-index pandas pyarrow\n",
    "!pip install langchain langchain-openai\n",
    "!pip install reportlab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dbb4",
   "metadata": {},
   "source": [
    "## 2) Configuraci√≥n de ruta y validaci√≥n del archivo\n",
    "\n",
    "- En este campo se debe colocar la ruta del log, de esta manera se valida su existencia y comprobaci√≥n de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e66bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# AJUSTA ESTA RUTA A TU ARCHIVO REAL, ya que sin esta comprobaci√≥n no es posible acceder a los datos.\n",
    "CSV_PATH = Path(\"TU_RUTA_DEL_LOG\")\n",
    "\n",
    "print(\"Archivo existe?:\", CSV_PATH.exists())\n",
    "print(\"Ruta:\", CSV_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5fca8fe-1a4b-4479-bd99-e6ac5c1f9d9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d483855",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Carga del archivo CSV para el analisis con DuckDB\n",
    "- Se usa esta libreria para hacer el tratamiento y procesamiento de los datos basados en el log, ya que por su peso de 1.1 GB los motores regulares no pueden procesarlo de manera directa. Para ello procedemos en la carga general tomando una muestra de los primeros 100.000 registros. De esta manera podemos cargar los datos no en un UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este fragmento del codigo, se usa la libreria y funcionalidad Duckdb, la cual nos permite procesar los datos del log como si fuera una vista de base de datos.\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "#  Ajusta a la ruta exacta del CSV (evita directorios archivos ocultos)\n",
    "CSV_FILE = Path(\"TU_RUTA_DEL_LOG\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Lee el CSV como todo VARCHAR e ignora bytes inv√°lidos (no UTF-8)\n",
    "rel = con.read_csv(\n",
    "    CSV_FILE.as_posix(),\n",
    "    header=True,\n",
    "    sample_size=100000,\n",
    "    ignore_errors=True,   # salta bytes/filas con codificaci√≥n mala\n",
    "    all_varchar=True,     # evita inferencia de tipos\n",
    "    delimiter=','\n",
    ")\n",
    "\n",
    "# Crea la vista 'logs' para tus queries SQL\n",
    "rel.create_view('logs')\n",
    "\n",
    "# Pruebas r√°pidas\n",
    "con.sql(\"SELECT * FROM logs LIMIT 5\").df()\n",
    "\n",
    "con.sql(\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT try_strptime(timestamp, '%Y-%d-%mT%H:%M:%S')    AS ts\n",
    "  FROM logs\n",
    "  UNION ALL\n",
    "  SELECT try_strptime(timestamp, '%Y-%d-%mT%H:%M')       AS ts\n",
    "  FROM logs\n",
    ")\n",
    "SELECT\n",
    "  strftime(ts, '%Y-%m') AS mes,      -- ejemplo: 2020-12\n",
    "  COUNT(*)              AS total_peticiones\n",
    "FROM t\n",
    "WHERE ts IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "\"\"\").df()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estructura de columnas detectadas\n",
    "con.sql(\"DESCRIBE logs\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51c0ea",
   "metadata": {},
   "source": [
    "## 4) Informaci√≥n general del log\n",
    "\n",
    "En este apartado se realiza la consulta general del Log, de esta manera podemos analizar los datos y entender el contexto del escenario propuesto. \n",
    "\n",
    "- Para esta investigaci√≥n fue necesario realizar diferentes cruces como tambien analisis de los datos, ya que al inicio se identifico cierta cantidad anomala de peticiones procedentes de diferentes sitios, dando a entender que el ataque podria tener diferentes naturalezas en comportamiento, es decir, explotaciones activas de IDOR con automatizaciones continuas sobre el API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faae657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total de peticiones\n",
    "con.sql(\"SELECT COUNT(*) AS total_peticiones FROM logs\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ad55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rango temporal\n",
    "con.sql(\"\"\"\n",
    "SELECT MIN(timestamp) AS fecha_inicio, MAX(timestamp) AS fecha_fin\n",
    "FROM logs\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9dc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top 20 IPs\n",
    "con.sql(\"\"\"\n",
    "SELECT source_ip, COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY source_ip\n",
    "ORDER BY total_peticiones DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# C√≥digos HTTP (nota: columna es http_staus en el CSV)\n",
    "con.sql(\"\"\"\n",
    "SELECT http_staus AS status, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_staus\n",
    "ORDER BY cantidad DESC\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# M√©todos\n",
    "con.sql(\"\"\"\n",
    "SELECT http_method, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_method\n",
    "ORDER BY cantidad DESC\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usuarios-agente (Top 20)\n",
    "con.sql(\"\"\"\n",
    "SELECT user_agent, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY user_agent\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Referers (Top 20)\n",
    "con.sql(\"\"\"\n",
    "SELECT http_host, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_host\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ffa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Serie por hora (0-23)\n",
    "con.sql(\"\"\"\n",
    "SELECT strftime(timestamp, '%H') AS hora, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY hora\n",
    "ORDER BY hora\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f9db6-91b2-4c57-a2c3-190816c318da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serie por pais\n",
    "\n",
    "con.sql(\"\"\"\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN http_uri LIKE '%site_id=MeliAR%' THEN 'MeliAR - Mercado Libre Argentina'\n",
    "    WHEN http_uri LIKE '%site_id=MeliBR%' THEN 'MeliBR - Mercado Libre Brasil'\n",
    "    WHEN http_uri LIKE '%site_id=MeliMX%' THEN 'MeliMX - Mercado Libre M√©xico'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCL%' THEN 'MeliCL - Mercado Libre Chile'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCO%' THEN 'MeliCO - Mercado Libre Colombia'\n",
    "    ELSE 'Otro / No especificado'\n",
    "  END AS pais,\n",
    "  COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%site_id=Meli%'\n",
    "GROUP BY 1\n",
    "ORDER BY total_peticiones DESC;\n",
    "\"\"\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Muestras de invoice_id y tokens m√°s consultados por esa IP\n",
    "con.sql(\"\"\"\n",
    "WITH q AS (\n",
    "  SELECT\n",
    "    regexp_extract(http_uri, 'invoice_id=([^&]+)', 1) AS invoice_id,\n",
    "    regexp_extract(http_uri, 'authtoken=([^&]+)', 1)  AS authtoken\n",
    "  FROM logs\n",
    "  WHERE http_uri LIKE '%/invoices/search%'\n",
    ")\n",
    "SELECT authtoken, COUNT(*) AS peticiones\n",
    "FROM q\n",
    "GROUP BY authtoken\n",
    "ORDER BY peticiones DESC\n",
    "\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba2c54",
   "metadata": {},
   "source": [
    "## 5) An√°lisis espec√≠fico con Agente IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97ccd9-33af-458e-9cce-2775c462a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Normalizador autom√°tico de columnas ‚Üí crea/actualiza vista `logs` ===\n",
    "import pandas as pd\n",
    "\n",
    "schema = con.sql(\"DESCRIBE rel\").df()\n",
    "cols_lower = {c.lower(): c for c in schema[\"column_name\"].tolist()}\n",
    "\n",
    "def resolve(*candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols_lower:\n",
    "            return cols_lower[c]\n",
    "    return None  # no existe\n",
    "\n",
    "# Detecta columnas reales\n",
    "c_timestamp   = resolve(\"timestamp\", \"time\", \"ts\", \"datetime\", \"date\")\n",
    "c_http_host   = resolve(\"http_host\", \"host\", \"domain\")\n",
    "c_http_uri    = resolve(\"http_uri\", \"uri\", \"path\", \"request_uri\", \"url\")\n",
    "c_http_status = resolve(\"http_status\", \"http_staus\", \"status\", \"status_code\", \"code\")\n",
    "c_http_method = resolve(\"http_method\", \"method\", \"verb\")\n",
    "c_user_agent  = resolve(\"user_agent\", \"http_user_agent\", \"ua\")\n",
    "c_client_ip   = resolve(\"client_ip\", \"source_ip\", \"ip\", \"remote_addr\", \"x_forwarded_for\")\n",
    "\n",
    "def sel(col, alias):\n",
    "    # si existe, √∫salo; si no, crea NULL::VARCHAR con alias para no romper\n",
    "    return f\"{col} AS {alias}\" if col else f\"NULL::VARCHAR AS {alias}\"\n",
    "\n",
    "select_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW logs AS\n",
    "SELECT\n",
    "  {sel(c_timestamp,   'timestamp')},\n",
    "  {sel(c_http_host,   'http_host')},\n",
    "  {sel(c_http_uri,    'http_uri')},\n",
    "  {sel(c_http_status, 'http_status')},\n",
    "  {sel(c_http_method, 'http_method')},\n",
    "  {sel(c_user_agent,  'user_agent')},\n",
    "  {sel(c_client_ip,   'client_ip')}\n",
    "FROM rel\n",
    "\"\"\"\n",
    "\n",
    "con.sql(select_sql)\n",
    "\n",
    "print(\"‚úÖ Vista `logs` creada/actualizada con columnas estandarizadas:\")\n",
    "print(con.sql(\"DESCRIBE logs\").df().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62705b31-4652-4b21-9e6e-b176cfb5b69c",
   "metadata": {},
   "source": [
    "## IMPORTANTE ESTABLECER API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4560072-28d8-45b1-bb0d-4aec3120a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Establecimiento API KEY\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"TU_API_KEY_AQUI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f27418-65d2-4a4c-81d8-fdd08570c0ea",
   "metadata": {},
   "source": [
    "- Para la creacion de este script, se utilizo langchain como fuente principal para el tratamiento de las LLM y poder generar las consultas a traves de OpenIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410825a-a87e-4fdd-aee8-9c270980c74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# El siguiente script permite realizar las siguientes acciones sobre la informacion cargada:\n",
    "# con an√°lisis completo de logs + Agente IA + PDF con tablas\n",
    "#  y agente con herramienta SQL consultando el 100% del log)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== Configuraci√≥n ==========\n",
    "CSV_FILE   = r\"TU_RUTA_LOG_AQUI\"  # Ruta confirmada\n",
    "OUTPUT_PDF = \"informe_forense_completo.pdf\"\n",
    "SAMPLE_MAX = 200   # Muestra opcional solo para contexto; el LLM puede consultar TODO con sql_agg\n",
    "\n",
    "# ========== Conexi√≥n DuckDB ==========\n",
    "try:\n",
    "    con  # si ya existe en el entorno interactivo\n",
    "except NameError:\n",
    "    con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "# ========== Helpers robustos ==========\n",
    "def table_exists(conn, name: str) -> bool:\n",
    "    \"\"\"Verifica existencia de tabla usando information_schema (sin lanzar excepci√≥n).\"\"\"\n",
    "    q = \"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema IN ('main','temp') AND table_name = ?\n",
    "    \"\"\"\n",
    "    return conn.execute(q, [name]).fetchone()[0] > 0\n",
    "\n",
    "def reload_rel_from_csv_streaming(conn, csv_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Crea/actualiza la tabla 'rel' leyendo el CSV en modo 'streaming' de DuckDB.\n",
    "    No carga todo en memoria; procesa directamente desde el archivo.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise RuntimeError(f\"No existe el CSV en la ruta: {csv_path}\")\n",
    "    print(f\"üì• (Streaming) Construyendo tabla 'rel' desde: {csv_path}\")\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE rel AS\n",
    "        SELECT * FROM read_csv_auto('{csv_path}', HEADER=TRUE);\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Tabla 'rel' creada/actualizada (streaming)\")\n",
    "\n",
    "def quoted_or_null(col: str) -> str:\n",
    "    return f'\"{col}\"' if col else \"NULL\"\n",
    "\n",
    "def df_or_empty(sql: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return con.execute(sql).df()\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def df_to_markdown_chunks(df: pd.DataFrame, chunk_size: int = 50, title: str = \"Tabla\"):\n",
    "    if df is None or df.empty:\n",
    "        return [f\"### {title}\\n_No hay datos disponibles._\\n\"]\n",
    "    chunks = []\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        sub = df.iloc[i:i+chunk_size]\n",
    "        header = f\"### {title} (filas {i+1}-{min(i+chunk_size, len(df))} de {len(df)})\"\n",
    "        chunks.append(header + \"\\n\" + sub.to_markdown(index=False) + \"\\n\")\n",
    "    return chunks\n",
    "\n",
    "def markdown_or_note(df: pd.DataFrame, title: str) -> str:\n",
    "    if df is None or df.empty:\n",
    "        return f\"### {title}\\n_No hay datos disponibles._\\n\"\n",
    "    return f\"### {title}\\n\" + df.to_markdown(index=False) + \"\\n\"\n",
    "\n",
    "# ========== Asegurar 'rel' ==========\n",
    "if not table_exists(con, \"rel\"):\n",
    "    reload_rel_from_csv_streaming(con, CSV_FILE)\n",
    "else:\n",
    "    print(\"‚úÖ La tabla 'rel' ya existe \")\n",
    "\n",
    "# Doble verificaci√≥n por si la carga fall√≥\n",
    "if not table_exists(con, \"rel\"):\n",
    "    raise RuntimeError(\n",
    "        \"No se encontr√≥ la tabla 'rel' tras intentar cargar el CSV. \"\n",
    "        \"Verifica la ruta, permisos y que el CSV no est√© vac√≠o/corrupto.\"\n",
    "    )\n",
    "\n",
    "# ========== Vista 'logs' segura con timestamp robusto ==========\n",
    "# Detectar columnas reales presentes\n",
    "cols_df = con.execute(\"\"\"\n",
    "SELECT column_name AS name\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema IN ('main','temp') AND table_name = 'rel'\n",
    "ORDER BY ordinal_position\n",
    "\"\"\").df()\n",
    "\n",
    "raw_cols = cols_df[\"name\"].tolist()\n",
    "lowmap   = {c.lower(): c for c in raw_cols}\n",
    "def get_col(*cands):\n",
    "    for c in cands:\n",
    "        x = lowmap.get(c.lower())\n",
    "        if x: return x\n",
    "    return None\n",
    "\n",
    "c_timestamp   = get_col(\"timestamp\",\"time\",\"datetime\",\"date\")\n",
    "c_http_host   = get_col(\"http_host\",\"host\",\"server_name\",\"authority\")\n",
    "c_http_uri    = get_col(\"http_uri\",\"request_uri\",\"uri\",\"path\",\"request\")\n",
    "c_http_status = get_col(\"http_status\",\"status\",\"code\",\"http_staus\")   # tolera typo\n",
    "c_http_method = get_col(\"http_method\",\"method\",\"verb\")\n",
    "c_user_agent  = get_col(\"http_user_agent\",\"user_agent\",\"ua\")\n",
    "c_client_ip   = get_col(\"client_ip\",\"source_ip\",\"remote_addr\",\"ip\",\"src_ip\")\n",
    "\n",
    "concat_expr = \" || ' ' || \".join([f'CAST(\"{c}\" AS VARCHAR)' for c in raw_cols]) if raw_cols else \"''\"\n",
    "q_ts = quoted_or_null(c_timestamp)\n",
    "\n",
    "# Normalizaci√≥n de timestamp (AAAA-DD-MMTHH:MM ‚Üí AAAA-MM-DDTHH:MM; + patrones alternos)\n",
    "create_logs_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW logs AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    COALESCE(\n",
    "      -- 1) Formato declarado: AAAA-DD-MMTHH:MM(:SS opcional)\n",
    "      try_strptime({q_ts}, '%Y-%d-%mT%H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%d-%mT%H:%M'),\n",
    "      -- 2) Reordenamos a AAAA-MM-DDTHH:MM(:SS) y parseamos est√°ndar\n",
    "      try_strptime(\n",
    "        REGEXP_REPLACE({q_ts}, '^(\\\\d{{4}})-(\\\\d{{2}})-(\\\\d{{2}})T', '\\\\1-\\\\3-\\\\2T'),\n",
    "        '%Y-%m-%dT%H:%M:%S'\n",
    "      ),\n",
    "      try_strptime(\n",
    "        REGEXP_REPLACE({q_ts}, '^(\\\\d{{4}})-(\\\\d{{2}})-(\\\\d{{2}})T', '\\\\1-\\\\3-\\\\2T'),\n",
    "        '%Y-%m-%dT%H:%M'\n",
    "      ),\n",
    "      -- 3) Otras variantes posibles (ISO, con Z, con espacio)\n",
    "      try_strptime({q_ts}, '%Y-%m-%dT%H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%m-%d %H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%m-%d'),\n",
    "      TRY_CAST({q_ts} AS TIMESTAMP)\n",
    "    ) AS timestamp,\n",
    "    {quoted_or_null(c_http_host)}   AS http_host,\n",
    "    {quoted_or_null(c_http_uri)}    AS http_uri,\n",
    "    {quoted_or_null(c_http_status)} AS http_status,\n",
    "    {quoted_or_null(c_http_method)} AS http_method,\n",
    "    {quoted_or_null(c_user_agent)}  AS user_agent,\n",
    "    {quoted_or_null(c_client_ip)}   AS client_ip,\n",
    "    REGEXP_EXTRACT({concat_expr}, '(?:\\\\d{{1,3}}\\\\.){{3}}\\\\d{{1,3}}', 0) AS derived_ip\n",
    "  FROM rel\n",
    ")\n",
    "SELECT\n",
    "  timestamp,\n",
    "  http_host,\n",
    "  http_uri,\n",
    "  http_status,\n",
    "  http_method,\n",
    "  user_agent,\n",
    "  COALESCE(client_ip, derived_ip) AS client_ip\n",
    "FROM base;\n",
    "\"\"\"\n",
    "con.execute(create_logs_sql)\n",
    "print(\"‚úÖ Vista 'logs'\")\n",
    "\n",
    "# ========== Datos clave de investigaci√≥n (100% del log) ==========\n",
    "df_timeline = df_or_empty(\"\"\"\n",
    "SELECT CAST(timestamp AS DATE) AS fecha, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "WHERE timestamp IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "df_status = df_or_empty(\"\"\"\n",
    "SELECT http_status AS status, COUNT(*) AS total\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "\"\"\")\n",
    "\n",
    "df_endpoints = df_or_empty(\"\"\"\n",
    "SELECT http_uri AS endpoint, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY peticiones DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "df_ua = df_or_empty(\"\"\"\n",
    "SELECT user_agent, COUNT(*) AS total\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "LIMIT 25\n",
    "\"\"\")\n",
    "\n",
    "# IPs (evidencia resumida)\n",
    "df_ip = df_or_empty(\"\"\"\n",
    "SELECT client_ip AS ip, COUNT(*) AS total\n",
    "FROM logs\n",
    "WHERE client_ip IS NOT NULL AND client_ip <> ''\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "# TODAS las IPs (para CSV; no se insertar√° en el PDF)\n",
    "df_ip_full = df_or_empty(\"\"\"\n",
    "SELECT client_ip AS ip, COUNT(*) AS total\n",
    "FROM logs\n",
    "WHERE client_ip IS NOT NULL AND client_ip <> ''\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "\"\"\")\n",
    "df_ip_full.to_csv(\"ips_conteo_completo.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Exportado CSV: ips_conteo_completo.csv\")\n",
    "\n",
    "# Heur√≠stica IDOR\n",
    "df_idor = df_or_empty(\"\"\"\n",
    "SELECT http_uri AS endpoint, COUNT(*) AS hits\n",
    "FROM logs\n",
    "WHERE http_uri ~ '(\\\\?|&)(id|uid|user|invoice|order|doc|account|acc)='\n",
    "   OR http_uri ~ '/\\\\d+'\n",
    "GROUP BY 1\n",
    "ORDER BY hits DESC\n",
    "LIMIT 500\n",
    "\"\"\")\n",
    "\n",
    "# Authtoken m√°s consultados (global) por /invoices/search\n",
    "df_authtoken_global = con.execute(\"\"\"\n",
    "WITH q AS (\n",
    "  SELECT\n",
    "    regexp_extract(http_uri, 'invoice_id=([^&]+)', 1) AS invoice_id,\n",
    "    regexp_extract(http_uri, 'authtoken=([^&]+)', 1)  AS authtoken\n",
    "  FROM logs\n",
    "  WHERE http_uri LIKE '%/invoices/search%'\n",
    ")\n",
    "SELECT authtoken, COUNT(*) AS peticiones\n",
    "FROM q\n",
    "GROUP BY authtoken\n",
    "ORDER BY peticiones DESC\n",
    "\"\"\").df()\n",
    "df_authtoken_global.to_csv(\"authtoken_global.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Exportado CSV: authtoken_global.csv\")\n",
    "\n",
    "# Volumen de peticiones por pa√≠s (site_id)\n",
    "df_por_pais = df_or_empty(\"\"\"\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN http_uri LIKE '%site_id=MeliAR%' THEN 'MeliAR - Mercado Libre Argentina'\n",
    "    WHEN http_uri LIKE '%site_id=MeliBR%' THEN 'MeliBR - Mercado Libre Brasil'\n",
    "    WHEN http_uri LIKE '%site_id=MeliMX%' THEN 'MeliMX - Mercado Libre M√©xico'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCL%' THEN 'MeliCL - Mercado Libre Chile'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCO%' THEN 'MeliCO - Mercado Libre Colombia'\n",
    "    ELSE 'Otro / No identificado'\n",
    "  END AS site_id,\n",
    "  COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total_peticiones DESC\n",
    "\"\"\")\n",
    "df_por_pais.to_csv(\"site_id_resumen.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Exportado CSV: site_id_resumen.csv\")\n",
    "\n",
    "# ========== Evidencia Markdown para IA ==========\n",
    "evidencia_md = \"\\n\".join([\n",
    "    \"*(Todas las tablas de esta secci√≥n se calcularon sobre el **100%** del log)*\\n\",\n",
    "    markdown_or_note(df_timeline,   \"Timeline diario\"),\n",
    "    markdown_or_note(df_por_pais,   \"Volumen de peticiones por pa√≠s (site_id)\"),\n",
    "    markdown_or_note(df_status,     \"Distribuci√≥n de c√≥digos de estado\"),\n",
    "    markdown_or_note(df_endpoints,  \"Endpoints m√°s solicitados\"),\n",
    "    markdown_or_note(df_ua,         \"User-Agents m√°s frecuentes\"),\n",
    "    markdown_or_note(df_ip,         \"Direcciones IP con mayor volumen (top 100)\"),\n",
    "    markdown_or_note(df_authtoken_global.head(50), \"Authtoken m√°s consultados ‚Äî /invoices/search (top 50)\"),\n",
    "    markdown_or_note(df_idor,       \"Patrones posibles de IDOR (heur√≠stica)\"),\n",
    "])\n",
    "\n",
    "# Muestra peque√±a opcional (el LLM PUEDE IGNORARLA; tiene acceso a sql_agg para 100%)\n",
    "df_logs_full = df_or_empty(\"SELECT * FROM logs\")\n",
    "if df_logs_full.empty:\n",
    "    sample_md = \"_El conjunto de logs est√° vac√≠o._\"\n",
    "else:\n",
    "    sample_n = min(len(df_logs_full), SAMPLE_MAX)\n",
    "    df_sample = df_logs_full.sample(sample_n, random_state=42) if len(df_logs_full) > sample_n else df_logs_full\n",
    "    sample_md = df_sample.head(SAMPLE_MAX).to_markdown(index=False)\n",
    "\n",
    "# ========== Agente IA (LangChain) ‚Äî con herramienta SQL sobre el 100% ==========\n",
    "respuesta_ia = None\n",
    "ia_error = None\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.agents import initialize_agent, AgentType\n",
    "    from langchain.tools import tool\n",
    "\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\n",
    "            \"No se encontr√≥ OPENAI_API_KEY en el entorno. \"\n",
    "            \"Configura la variable de entorno antes de ejecutar este script.\"\n",
    "        )\n",
    "\n",
    "    # Herramienta segura para consultas SELECT sobre TODO el log.\n",
    "    @tool(\"sql_agg\", return_direct=False)\n",
    "    def sql_agg(sql: str) -> str:\n",
    "        \"\"\"\n",
    "        Ejecuta consultas SQL **solo-lectura (SELECT)** sobre la base completa (100% del log).\n",
    "        Devuelve el resultado en Markdown. Limita a 2000 filas para no exceder el tama√±o.\n",
    "        Seguridad:\n",
    "          - Rechaza consultas que no comiencen por 'SELECT' (ignorando espacios/lineas).\n",
    "          - Rechaza palabras peligrosas: CREATE/INSERT/UPDATE/DELETE/ALTER/DROP/REPLACE/ATTACH/COPY.\n",
    "        \"\"\"\n",
    "        if not isinstance(sql, str) or not sql.strip():\n",
    "            return \"‚ö†Ô∏è sql_agg: Debes enviar una consulta SELECT no vac√≠a.\"\n",
    "        sql_clean = sql.strip().lstrip(\"(\\n\\t \").upper()\n",
    "        if not sql_clean.startswith(\"SELECT\"):\n",
    "            return \"‚õî sql_agg: Solo se permiten consultas SELECT.\"\n",
    "        forbidden = (\"CREATE\",\"INSERT\",\"UPDATE\",\"DELETE\",\"ALTER\",\"DROP\",\"REPLACE\",\"ATTACH\",\"COPY\",\"PRAGMA\",\"IMPORT\",\"EXPORT\",\"LOAD\")\n",
    "        if any(w in sql_clean for w in forbidden):\n",
    "            return \"‚õî sql_agg: Consulta rechazada por contener palabras no permitidas.\"\n",
    "        try:\n",
    "            df = con.execute(sql).df()\n",
    "            if df.empty:\n",
    "                return \"_(sin filas)_\"\n",
    "            # Limitar filas para respuesta\n",
    "            if len(df) > 2000:\n",
    "                df = df.head(2000)\n",
    "                note = \"\\n_(resultado truncado a 2000 filas)_\\n\"\n",
    "            else:\n",
    "                note = \"\"\n",
    "            return df.to_markdown(index=False) + note\n",
    "        except Exception as e:\n",
    "            return f\" Error ejecutando SQL: {e}\"\n",
    "\n",
    "    @tool(\"get_aggregates_md\", return_direct=False)\n",
    "    def get_aggregates_md(_: str = \"\") -> str:\n",
    "        \"\"\"Devuelve las tablas agregadas (100% del log) en Markdown.\"\"\"\n",
    "        return evidencia_md\n",
    "\n",
    "    @tool(\"get_sample_md\", return_direct=False)\n",
    "    def get_sample_md(_: str = \"\") -> str:\n",
    "        \"\"\"Devuelve una muestra representativa peque√±a del log en Markdown (opcional).\"\"\"\n",
    "        return sample_md\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    tools = [sql_agg, get_aggregates_md, get_sample_md]\n",
    "\n",
    "    try:\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Eres un analista forense experto. Todo el **contexto agregado** proviene del **100% del log**.\n",
    "Si necesitas verificar cualquier detalle, puedes usar la herramienta **sql_agg** para ejecutar\n",
    "consultas SQL *SELECT* directamente sobre el conjunto de datos completo (100% del log).\n",
    "\n",
    "## Evidencia agregada (100% del log; tablas)\n",
    "{evidencia_md}\n",
    "\n",
    "## Muestra opcional (solo referencia r√°pida; puedes ignorarla si usas sql_agg)\n",
    "{sample_md}\n",
    "\n",
    "Con base en TODO el log (usa preferentemente los agregados del 100% y, si hace falta,\n",
    "consulta con sql_agg), elabora un **informe forense completo en Markdown** con la siguiente estructura y formato:\n",
    "\n",
    "## Resumen ejecutivo\n",
    "- Contexto del incidente, alcance e impacto potencial, partiendo del LOG completo. Indica la vulnerabilidad predominante si la evidencias.\n",
    "\n",
    "## Narrativa del incidente\n",
    "- L√≠nea de tiempo de eventos relevantes y fases del ataque (usa timeline y/o agr√©galo con sql_agg si requieres granularidad).\n",
    "\n",
    "## Indicadores de compromiso (IoC)\n",
    "- Direcciones IP, endpoints, user-agents y comportamientos an√≥malos. Presenta **tablas** cuando corresponda.\n",
    "\n",
    "## Posibles patrones IDOR\n",
    "- Evidencias de manipulaci√≥n de par√°metros y accesos indebidos (usa **tablas** y justifica por qu√© podr√≠an ser IDOR).\n",
    "\n",
    "## Estad√≠sticas clave del incidente\n",
    "- **Tablas Markdown** con la distribuci√≥n de status, endpoints m√°s atacados, volumen por IP, site_id/pa√≠s, etc.\n",
    "\n",
    "## An√°lisis de User-Agents\n",
    "- Destaca bots o automatizaciones (wget, curl, python, scrapy, spider, bot) con **tablas**.\n",
    "\n",
    "## Recomendaciones\n",
    "- Acciones inmediatas y mitigaciones a mediano y largo plazo, detalladas.\n",
    "\n",
    "### Reglas estrictas de formato\n",
    "- Usa **t√≠tulos H2/H3** consistentes.\n",
    "- Donde presentes datos, usa **tablas Markdown** (| col1 | col2 | ... |).\n",
    "- **NO RESUMAS** filas de tablas: si una tabla es larga, div√≠dela en bloques de 50 filas, **sin omitir filas**.\n",
    "- No incluyas trazas ni llamadas a herramientas en la respuesta (solo resultados).\n",
    "\"\"\"\n",
    "    respuesta_ia = agent.run(prompt)\n",
    "    print(\"Respuesta IA (vista previa):\\n\", (respuesta_ia or \"\")[:1500])\n",
    "\n",
    "except Exception as e:\n",
    "    ia_error = repr(e)\n",
    "    print(\"‚ÑπÔ∏è No se pudo ejecutar el agente IA:\", ia_error)\n",
    "    partes = [\n",
    "        \"# Informe Forense (sin IA)\",\n",
    "        \"## Resumen\",\n",
    "        \"No se pudo ejecutar el modelo. Se incluyen agregados para referencia (100% del log).\",\n",
    "        evidencia_md\n",
    "    ]\n",
    "    respuesta_ia = \"\\n\\n\".join(partes)\n",
    "\n",
    "# ========== Secci√≥n fija adicional en el PDF ==========\n",
    "# 1) Authtoken completos (en bloques para no truncar)\n",
    "authtoken_chunks = df_to_markdown_chunks(\n",
    "    df_authtoken_global,\n",
    "    chunk_size=50,\n",
    "    title=\"Authtoken m√°s consultados ‚Äî /invoices/search (completo)\"\n",
    ")\n",
    "seccion_authtoken = \"\\n\".join(authtoken_chunks)\n",
    "\n",
    "# 2) Volumen de peticiones por pa√≠s (site_id) ‚Äî secci√≥n solicitada\n",
    "site_id_chunks = df_to_markdown_chunks(\n",
    "    df_por_pais,\n",
    "    chunk_size=50,\n",
    "    title=\"Volumen de peticiones por pa√≠s (site_id)\"\n",
    ")\n",
    "seccion_site_id = \"\\n\".join(site_id_chunks)\n",
    "\n",
    "# Unir secciones fijas al informe final\n",
    "respuesta_ia = (\n",
    "    (respuesta_ia or \"# Informe Forense\")\n",
    "    + \"\\n\\n## Secci√≥n fija ‚Äî Volumen de peticiones por pa√≠s (site_id)\\n\"\n",
    "    + seccion_site_id\n",
    "    + \"\\n\\n> Archivo complementario: `site_id_resumen.csv`\\n\"\n",
    "    + \"\\n\\n## Secci√≥n fija ‚Äî Authtoken m√°s consultados\\n\"\n",
    "    + seccion_authtoken\n",
    "    + \"\\n\\n> Archivo complementario: `authtoken_global.csv`\"\n",
    ")\n",
    "\n",
    "# ========== Conversi√≥n Markdown -> PDF (ReportLab con auto-wrap de celdas) ==========\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib import colors\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, ListFlowable, ListItem\n",
    "    from reportlab.lib.units import cm\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    STY_H1 = ParagraphStyle(\"H1\", parent=styles[\"Heading1\"], fontSize=16, leading=20, spaceAfter=10)\n",
    "    STY_H2 = ParagraphStyle(\"H2\", parent=styles[\"Heading2\"], fontSize=14, leading=18, spaceAfter=8)\n",
    "    STY_H3 = ParagraphStyle(\"H3\", parent=styles[\"Heading3\"], fontSize=12, leading=16, spaceAfter=6)\n",
    "    STY_BODY = ParagraphStyle(\"BODY\", parent=styles[\"BodyText\"], fontSize=10.0, leading=13.5, spaceAfter=6)\n",
    "\n",
    "    def parse_md_tables(md_text: str):\n",
    "        \"\"\"Devuelve lista de bloques: {'type': 'table'|'heading'|'list'|'p', ...}\"\"\"\n",
    "        lines = md_text.splitlines()\n",
    "        blocks, buf_para = [], []\n",
    "\n",
    "        def flush_para():\n",
    "            if buf_para:\n",
    "                blocks.append({\"type\":\"p\", \"text\":\"\\n\".join(buf_para).strip()})\n",
    "                buf_para.clear()\n",
    "\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].rstrip()\n",
    "\n",
    "            # Encabezados\n",
    "            if re.match(r\"^#{1,6}\\s+\", line):\n",
    "                flush_para()\n",
    "                level = len(line) - len(line.lstrip(\"#\"))\n",
    "                text = line[level:].strip()\n",
    "                blocks.append({\"type\":\"heading\", \"level\":level, \"text\":text})\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Listas\n",
    "            if re.match(r\"^\\s*[-*]\\s+\", line):\n",
    "                flush_para()\n",
    "                items = []\n",
    "                while i < len(lines) and re.match(r\"^\\s*[-*]\\s+\", lines[i]):\n",
    "                    items.append(re.sub(r\"^\\s*[-*]\\s+\", \"\", lines[i]).strip())\n",
    "                    i += 1\n",
    "                blocks.append({\"type\":\"list\", \"items\":items})\n",
    "                continue\n",
    "\n",
    "            # Tablas Markdown\n",
    "            if '|' in line:\n",
    "                if i+1 < len(lines) and re.match(r'^\\s*\\|?[:\\-| ]+\\|?\\s*$', lines[i+1]):\n",
    "                    tbl_lines = [line, lines[i+1]]\n",
    "                    i += 2\n",
    "                    while i < len(lines) and '|' in lines[i] and lines[i].strip():\n",
    "                        tbl_lines.append(lines[i])\n",
    "                        i += 1\n",
    "                    rows = []\n",
    "                    for idx, tline in enumerate(tbl_lines):\n",
    "                        parts = [c.strip() for c in tline.strip().strip('|').split('|')]\n",
    "                        if idx == 1:\n",
    "                            continue  # separador\n",
    "                        rows.append(parts)\n",
    "                    if rows:\n",
    "                        flush_para()\n",
    "                        blocks.append({\"type\":\"table\", \"rows\":rows})\n",
    "                    continue\n",
    "\n",
    "            # P√°rrafos\n",
    "            if line.strip() == \"\":\n",
    "                flush_para()\n",
    "            else:\n",
    "                buf_para.append(line)\n",
    "            i += 1\n",
    "\n",
    "        flush_para()\n",
    "        return blocks\n",
    "\n",
    "    # ====== AJUSTE DE TABLAS: ancho m√°ximo + auto-wrap en celdas ======\n",
    "    from reportlab.platypus import Table, TableStyle, Paragraph\n",
    "    from reportlab.lib.units import cm\n",
    "\n",
    "    def md_to_story(md_text: str):\n",
    "        blocks = parse_md_tables(md_text or \"\")\n",
    "        story = []\n",
    "        # Portada\n",
    "        story.append(Paragraph(\"Informe Forense Automatizado\", STY_H1))\n",
    "        story.append(Paragraph(datetime.now().strftime(\"%Y-%m-%d %H:%M\"), STY_BODY))\n",
    "        story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == \"heading\":\n",
    "                sty = STY_H1 if b[\"level\"] <= 1 else STY_H2 if b[\"level\"] == 2 else STY_H3\n",
    "                story.append(Paragraph(b[\"text\"], sty))\n",
    "\n",
    "            elif b[\"type\"] == \"list\":\n",
    "                items = [ListItem(Paragraph(it.replace(\"\\n\",\"<br/>\"), STY_BODY)) for it in b[\"items\"]]\n",
    "                story.append(ListFlowable(items, bulletType=\"bullet\"))\n",
    "\n",
    "            elif b[\"type\"] == \"table\":\n",
    "                data = b[\"rows\"]\n",
    "\n",
    "                # Limitar ancho m√°ximo de tabla: A4(21cm) - m√°rgenes (2+2) = 17cm\n",
    "                max_table_width = 17 * cm\n",
    "                num_cols = max(len(data[0]), 1)\n",
    "                col_width = max_table_width / num_cols\n",
    "\n",
    "                # Envolver texto en celdas largas con saltos de l√≠nea\n",
    "                wrapped_data = []\n",
    "                for row in data:\n",
    "                    wrapped_row = []\n",
    "                    for cell in row:\n",
    "                        text = str(cell)\n",
    "                        # Si la celda es muy larga, inserta saltos cada ~60 chars\n",
    "                        if len(text) > 60:\n",
    "                            text = \"<br/>\".join([text[i:i+60] for i in range(0, len(text), 60)])\n",
    "                        wrapped_row.append(Paragraph(text, STY_BODY))\n",
    "                    wrapped_data.append(wrapped_row)\n",
    "\n",
    "                t = Table(wrapped_data, colWidths=[col_width]*num_cols, repeatRows=1)\n",
    "                t.setStyle(TableStyle([\n",
    "                    ('FONTNAME',(0,0),(-1,-1),'Helvetica'),\n",
    "                    ('FONTSIZE',(0,0),(-1,-1),8.5),\n",
    "                    ('BACKGROUND',(0,0),(-1,0),colors.HexColor(\"#F0F2F5\")),\n",
    "                    ('TEXTCOLOR',(0,0),(-1,0),colors.black),\n",
    "                    ('GRID',(0,0),(-1,-1),0.3,colors.HexColor(\"#B8C1CC\")),\n",
    "                    ('ALIGN',(0,0),(-1,0),'CENTER'),\n",
    "                    ('ALIGN',(0,1),(-1,-1),'LEFT'),\n",
    "                    ('VALIGN',(0,0),(-1,-1),'TOP'),\n",
    "                    ('ROWBACKGROUNDS',(0,1),(-1,-1),[colors.white, colors.HexColor(\"#FCFEFF\")]),\n",
    "                    ('TOPPADDING',(0,0),(-1,-1),3),\n",
    "                    ('BOTTOMPADDING',(0,0),(-1,-1),3),\n",
    "                ]))\n",
    "                story.append(t)\n",
    "\n",
    "            else:  # p√°rrafo\n",
    "                text = b[\"text\"].replace(\"&\", \"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\")\n",
    "                story.append(Paragraph(text.replace(\"\\n\",\"<br/>\"), STY_BODY))\n",
    "\n",
    "            story.append(Spacer(1, 0.25*cm))\n",
    "        return story\n",
    "\n",
    "    from reportlab.platypus import SimpleDocTemplate\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "\n",
    "    doc = SimpleDocTemplate(\n",
    "        OUTPUT_PDF, pagesize=A4,\n",
    "        topMargin=1.5*cm, bottomMargin=1.5*cm, leftMargin=2*cm, rightMargin=2*cm\n",
    "    )\n",
    "    story = md_to_story(respuesta_ia or \"# Informe vac√≠o\")\n",
    "    doc.build(story)\n",
    "    print(f\"‚úÖ PDF generado: {os.path.abspath(OUTPUT_PDF)}\")\n",
    "    print(\"CSVs generados: ips_conteo_completo.csv, authtoken_global.csv, site_id_resumen.csv\")\n",
    "    if ia_error:\n",
    "        print(\" Nota: el PDF se gener√≥, pero la IA fall√≥ y se us√≥ contenido de respaldo.\")\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Falta una librer√≠a para exportar PDF:\", e)\n",
    "    print(\"Instala con:  pip install reportlab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fb98d-dc9d-4838-9bd7-9c549f98a3f5",
   "metadata": {},
   "source": [
    "## 6) ALMACENADOR DE MD PARA USO DE CHAT INTERACTIVO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541a692-de53-407a-af19-ceeaf7b04a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Guardar el informe en Markdown, es necesario este paso para poder usar el el chat interactivo, este se genera a traves del uso de la anterior celda.\n",
    "# - Usa la salida real del agente (respuesta_ia) si existe\n",
    "# - Si no, guarda un respaldo con la evidencia agregada\n",
    "# - Normaliza saltos de l√≠nea, asegura UTF-8 y crea una versi√≥n con timestamp\n",
    "# ================================================\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "def _safe_text(s) -> str:\n",
    "    \"\"\"Convierte a str, normaliza saltos de l√≠nea y asegura fin de archivo con newline.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    # normaliza CRLF/CR a LF\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # quita null bytes por si vinieran de alguna conversi√≥n extra√±a\n",
    "    s = s.replace(\"\\x00\", \"\")\n",
    "    # asegura 1 salto de l√≠nea final\n",
    "    if not s.endswith(\"\\n\"):\n",
    "        s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "def _build_fallback_md() -> str:\n",
    "    partes = [\n",
    "        \"# Informe Forense (respaldo)\\n\",\n",
    "        f\"_Generado autom√°ticamente: {datetime.now().strftime('%Y-%m-%d %H:%M')}_\\n\",\n",
    "        \"## Nota\\nNo se obtuvo salida del modelo IA. Se incluyen agregados para referencia.\\n\",\n",
    "    ]\n",
    "    ev = globals().get(\"evidencia_md\")\n",
    "    if isinstance(ev, str) and ev.strip():\n",
    "        partes.append(ev)\n",
    "    else:\n",
    "        partes.append(\"_No hay evidencia agregada disponible._\")\n",
    "    return \"\\n\\n\".join(partes)\n",
    "\n",
    "# 1) Determinar el contenido a guardar\n",
    "contenido_md = None\n",
    "resp = globals().get(\"respuesta_ia\")\n",
    "if isinstance(resp, str) and resp.strip():\n",
    "    contenido_md = resp\n",
    "else:\n",
    "    contenido_md = _build_fallback_md()\n",
    "\n",
    "contenido_md = _safe_text(contenido_md)\n",
    "\n",
    "# 2) Construir nombre de salida\n",
    "#    - base: informe_forense_completo.md (compatible con tu flujo)\n",
    "#    - copia con timestamp para trazabilidad\n",
    "base_name = \"informe_forense_completo\"\n",
    "OUTPUT_MD = f\"{base_name}.md\"\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_MD_TS = f\"{base_name}_{ts}.md\"\n",
    "\n",
    "# 3) (Opcional) Inyectar metadatos m√≠nimos al inicio si no existen encabezados\n",
    "if not contenido_md.lstrip().startswith(\"# \"):\n",
    "    meta = [\n",
    "        f\"# Informe Forense Automatizado\",\n",
    "        f\"_Generado: {datetime.now().strftime('%Y-%m-%d %H:%M')}_\",\n",
    "        \"\",\n",
    "    ]\n",
    "    contenido_md = _safe_text(\"\\n\".join(meta) + contenido_md)\n",
    "\n",
    "# 4) Escribir a disco (UTF-8) en dos archivos: el ‚Äúcanonical‚Äù y la copia con timestamp\n",
    "def _write_utf8(path: str, text: str):\n",
    "    # Usa io.open para asegurar comportamiento consistente en distintos entornos\n",
    "    with io.open(path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "try:\n",
    "    _write_utf8(OUTPUT_MD, contenido_md)\n",
    "    _write_utf8(OUTPUT_MD_TS, contenido_md)\n",
    "\n",
    "    # 5) Mensajes de confirmaci√≥n √∫tiles\n",
    "    abs_main = os.path.abspath(OUTPUT_MD)\n",
    "    abs_ts   = os.path.abspath(OUTPUT_MD_TS)\n",
    "\n",
    "    size_main = os.path.getsize(OUTPUT_MD)\n",
    "    size_ts   = os.path.getsize(OUTPUT_MD_TS)\n",
    "\n",
    "    print(\"‚úÖ Informe Markdown guardado correctamente.\")\n",
    "    print(f\"   ‚Ä¢ Principal : {abs_main}  ({size_main:,} bytes)\")\n",
    "    print(f\"   ‚Ä¢ Timestamp : {abs_ts}  ({size_ts:,} bytes)\")\n",
    "\n",
    "    # 6) Consejos de integraci√≥n con tu flujo actual\n",
    "    #    - Si usas el chat forense interactivo, √©ste buscar√° 'informe_forense_completo.md' por defecto.\n",
    "    #    - La copia con timestamp te permite versionar r√°pidamente entregables.\n",
    "\n",
    "except Exception as e:\n",
    "    # Falla segura con explicaci√≥n\n",
    "    print(\"‚õî Error al guardar el informe en Markdown:\", e, file=sys.stderr)\n",
    "    print(\"   Verifica permisos de escritura y rutas.\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75318908-c0fa-43ce-8a50-a0421009c109",
   "metadata": {},
   "source": [
    "## 7) CHAT INTERACTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acc760-f3cf-4930-9807-a6a8a191adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Chat\n",
    "# - Carga robusta del informe Markdown\n",
    "# - QA con LLM usando el informe como contexto (recorte inteligente)\n",
    "# - B√∫squeda de fragmentos relevantes en el informe\n",
    "# - Presets de preguntas forenses frecuentes\n",
    "# ================================================================\n",
    "\n",
    "import os, re, textwrap, pathlib\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# ---------- 1) Localizaci√≥n robusta del informe ----------\n",
    "def _find_latest_report(base_name=\"informe_forense_completo\", exts=(\".md\",)):\n",
    "    \"\"\"Devuelve la ruta del informe m√°s reciente.\"\"\"\n",
    "    candidates = []\n",
    "    cwd = pathlib.Path(\".\")\n",
    "    for ext in exts:\n",
    "        candidates += list(cwd.glob(f\"{base_name}{ext}\"))\n",
    "        candidates += list(cwd.glob(f\"{base_name}_*.{ext.strip('.')}\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return str(candidates[0])\n",
    "\n",
    "if \"out_md\" in globals() and isinstance(out_md, str) and os.path.exists(out_md):\n",
    "    INFORME_PATH = out_md\n",
    "else:\n",
    "    INFORME_PATH = _find_latest_report() or \"informe_forense_completo.md\"\n",
    "\n",
    "if not os.path.exists(INFORME_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"No encontr√© el informe: {INFORME_PATH}\\n\"\n",
    "        \"Aseg√∫rate de haber generado el Markdown o ajusta INFORME_PATH.\"\n",
    "    )\n",
    "\n",
    "with open(INFORME_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    INFORME_MD = f.read()\n",
    "\n",
    "# ---------- 2) Utilidades de contexto ----------\n",
    "def _trim_context_sections(md: str, max_chars: int = 120_000) -> str:\n",
    "    \"\"\"Recorte inteligente manteniendo las secciones m√°s relevantes.\"\"\"\n",
    "    md = md.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    if len(md) <= max_chars:\n",
    "        return md\n",
    "\n",
    "    parts = re.split(r'(?m)^#{2,3}\\s+', md)\n",
    "    headers = re.findall(r'(?m)^#{2,3}\\s+(.+)$', md)\n",
    "    sections = []\n",
    "    lead = parts[0]\n",
    "    for h, body in zip(headers, parts[1:]):\n",
    "        sections.append((h.strip(), body))\n",
    "\n",
    "    signals = re.compile(\n",
    "        r'\\b(401|403|404|500|SQL|IDOR|token|authtoken|invoice|/invoices|login|auth|csrf|xss|timeout|error|bot|curl|wget|python|spider|scrapy|endpoint|site_id|ip|user-agent)\\b',\n",
    "        re.I\n",
    "    )\n",
    "    scored = []\n",
    "    for h, body in sections:\n",
    "        score = len(signals.findall(body)) + len(signals.findall(h))\n",
    "        scored.append((score, h, body))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    keep = []\n",
    "    total = len(lead)\n",
    "    keep.append(lead)\n",
    "    i = 0\n",
    "    while i < len(scored) and total < max_chars * 0.85:\n",
    "        score, h, body = scored[i]\n",
    "        chunk = f\"\\n\\n## {h}\\n{body}\"\n",
    "        total += len(chunk)\n",
    "        keep.append(chunk)\n",
    "        i += 1\n",
    "\n",
    "    tail = md[-min(len(md)//6, int(max_chars*0.15)):]\n",
    "    keep.append(\"\\n\\n...[contexto recortado]...\\n\\n\")\n",
    "    keep.append(tail)\n",
    "    return \"\".join(keep)\n",
    "\n",
    "CTX_FULL = INFORME_MD\n",
    "CTX = _trim_context_sections(INFORME_MD)\n",
    "\n",
    "def _context_snippets(query: str, md: str, window=220, max_hits=6) -> List[str]:\n",
    "    \"\"\"Extrae fragmentos relevantes del informe.\"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return []\n",
    "    toks = [re.escape(t) for t in re.findall(r'\\w{3,}', query.lower())]\n",
    "    if not toks:\n",
    "        return []\n",
    "    pat = re.compile(\"|\".join(toks), re.I)\n",
    "    hits = []\n",
    "    for m in pat.finditer(md):\n",
    "        start = max(0, m.start() - window)\n",
    "        end = min(len(md), m.end() + window)\n",
    "        frag = md[start:end].replace(\"\\n\", \" \")\n",
    "        hits.append(\"‚Ä¶ \" + frag + \" ‚Ä¶\")\n",
    "        if len(hits) >= max_hits:\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "# ---------- 3) LLM (LangChain + OpenAI) ----------\n",
    "LLM_READY = True\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "except Exception as e:\n",
    "    LLM_READY = False\n",
    "    _IMPORT_ERR = repr(e)\n",
    "\n",
    "if LLM_READY:\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    except Exception as e:\n",
    "        LLM_READY = False\n",
    "        _INIT_ERR = repr(e)\n",
    "\n",
    "# ---------- 4) Di√°logo forense ----------\n",
    "def dialogo_forense(pregunta: str, incluir_snippets: bool = True) -> str:\n",
    "    \"\"\"Pregunta a la IA usando el informe forense como contexto.\"\"\"\n",
    "    if not pregunta or not pregunta.strip():\n",
    "        return \"Por favor escribe una pregunta.\"\n",
    "\n",
    "    snippets = _context_snippets(pregunta, CTX_FULL) if incluir_snippets else []\n",
    "\n",
    "    if not LLM_READY:\n",
    "        detalles = []\n",
    "        if \"_IMPORT_ERR\" in globals(): detalles.append(f\"ImportError: {_IMPORT_ERR}\")\n",
    "        if \"_INIT_ERR\" in globals():   detalles.append(f\"InitError: {_INIT_ERR}\")\n",
    "        extra = (\"\\n\\nDetalles: \" + \" | \".join(detalles)) if detalles else \"\"\n",
    "        base = (\n",
    "            \"‚ö†Ô∏è El modelo de IA no est√° disponible. \"\n",
    "            \"Verifica tu API Key y que `langchain-openai` est√© instalado.\"\n",
    "            + extra\n",
    "        )\n",
    "        if snippets:\n",
    "            base += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return base\n",
    "\n",
    "    instruct = (\n",
    "        \"Eres un analista forense experto. Responde en espa√±ol, claro y accionable. \"\n",
    "        \"Si algo no est√° en el contexto, dilo expl√≠citamente y sugiere qu√© dato falta.\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", instruct),\n",
    "        (\"system\", f\"Contexto del informe (Markdown recortado):\\n{CTX}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "    try:\n",
    "        msg = prompt.format_messages(question=pregunta)\n",
    "        ans = llm.invoke(msg)\n",
    "        out = ans.content if hasattr(ans, \"content\") else str(ans)\n",
    "\n",
    "        if snippets:\n",
    "            out += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        base = (\n",
    "            \"‚ö†Ô∏è No pude obtener respuesta del modelo.\\n\\n\"\n",
    "            f\"Error: {e}\\n\\n\"\n",
    "            \"Sugerencias: prueba con una pregunta m√°s corta o revisa los l√≠mites de tokens.\"\n",
    "        )\n",
    "        if snippets:\n",
    "            base += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return base\n",
    "\n",
    "# ---------- 5) UI con ipywidgets ----------\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "PRESETS = [\n",
    "    \"Resumen del incidente\",\n",
    "    \"Principales indicadores de compromiso\",\n",
    "    \"Patrones IDOR detectados\",\n",
    "    \"Recomendaciones inmediatas\",\n",
    "    \"An√°lisis de User-Agents sospechosos\",\n",
    "    \"Fases del ataque en la l√≠nea de tiempo\",\n",
    "]\n",
    "\n",
    "inp_q = widgets.Textarea(\n",
    "    placeholder='Haz una pregunta sobre la investigaci√≥n forense‚Ä¶',\n",
    "    description='Pregunta:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "chk_snip = widgets.Checkbox(value=True, description=\"Incluir fragmentos del informe\")\n",
    "ddl_preset = widgets.Dropdown(options=PRESETS, description=\"Preguntas r√°pidas:\")\n",
    "btn = widgets.Button(description=\"Preguntar\", button_style='primary', icon=\"search\")\n",
    "out = widgets.Output(layout={'border': '1px solid #ddd'})\n",
    "\n",
    "def _on_preset_change(change):\n",
    "    if change['name'] == 'value':\n",
    "        inp_q.value = f\"Expl√≠came: {change['new']}\"\n",
    "\n",
    "ddl_preset.observe(_on_preset_change, names='value')\n",
    "\n",
    "def _on_click(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(\"üß† Respuesta:\")\n",
    "        resp = dialogo_forense(inp_q.value, incluir_snippets=chk_snip.value)\n",
    "        display(Markdown(resp))\n",
    "\n",
    "btn.on_click(_on_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    inp_q,\n",
    "    chk_snip,\n",
    "    ddl_preset,\n",
    "    btn,\n",
    "    widgets.HTML(\"<b>üí¨ Respuesta:</b>\"),\n",
    "    out\n",
    "]))\n",
    "\n",
    "print(f\"üìÑ Informe cargado: {os.path.abspath(INFORME_PATH)}\")\n",
    "print(f\"ü™™ LLM listo: {LLM_READY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01843f55-a158-46de-887e-0def855b4f2d",
   "metadata": {},
   "source": [
    "## GRACIAS POR LA ATENCION PRESTADA!!!\n",
    "\n",
    "- **NOTA:El notebook, esta organizado para que sea ejecutado celda por celda, de esta manera se garantiza la linealidad del funcionamiento del codigo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0b757-f37b-4f6e-8f01-699f6a4ae60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
