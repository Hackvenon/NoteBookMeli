{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bed36ae",
   "metadata": {},
   "source": [
    "\n",
    "# INFORME TECNICO DETALLADO - EXPLOTACION IDOR - MELI\n",
    "# Prueba Tecnica Meli - 2025\n",
    "# Andres Mateo Diaz Cañon\n",
    "\n",
    "El presente informe documenta el análisis realizado sobre los registros de red proporcionados en el archivo “three_months_logs.csv”. El objetivo fue determinar la existencia de actividad anómala vinculada a una vulnerabilidad de tipo Insecure Direct Object Reference (IDOR) en el endpoint /invoices/search de www.mercadolibre.com.\n",
    "\n",
    "> **Importante:** Cambia `CSV_FILE` si tu ruta es distinta, y ajusta `delimiter` si tu CSV usa `;`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65994f9",
   "metadata": {},
   "source": [
    "## 1) Requisitos y  Verificacion de complementos\n",
    "\n",
    "Para el funcionamiento correcto del Notebook, se requiere la instalacion de los siguientes paquetes y librerias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Requisitos básicos\n",
    "!python -m pip install langchain duckdb openai llama-index pandas pyarrow\n",
    "!pip install langchain langchain-openai\n",
    "!pip install reportlab\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5dbb4",
   "metadata": {},
   "source": [
    "## 2) Configuración de ruta y validación del archivo\n",
    "\n",
    "- En este campo se debe colocar la ruta del log, de esta manera se valida su existencia y comprobación de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e66bf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# AJUSTA ESTA RUTA A TU ARCHIVO REAL, ya que sin esta comprobación no es posible acceder a los datos.\n",
    "CSV_PATH = Path(\"TU_RUTA_DEL_LOG\")\n",
    "\n",
    "print(\"Archivo existe?:\", CSV_PATH.exists())\n",
    "print(\"Ruta:\", CSV_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5fca8fe-1a4b-4479-bd99-e6ac5c1f9d9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d483855",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Carga del archivo CSV para el analisis con DuckDB\n",
    "- Se usa esta libreria para hacer el tratamiento y procesamiento de los datos basados en el log, ya que por su peso de 1.1 GB los motores regulares no pueden procesarlo de manera directa. Para ello procedemos en la carga general tomando una muestra de los primeros 100.000 registros. De esta manera podemos cargar los datos no en un UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este fragmento del codigo, se usa la libreria y funcionalidad Duckdb, la cual nos permite procesar los datos del log como si fuera una vista de base de datos.\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "#  Ajusta a la ruta exacta del CSV (evita directorios archivos ocultos)\n",
    "CSV_FILE = Path(\"TU_RUTA_DEL_LOG\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Lee el CSV como todo VARCHAR e ignora bytes inválidos (no UTF-8)\n",
    "rel = con.read_csv(\n",
    "    CSV_FILE.as_posix(),\n",
    "    header=True,\n",
    "    sample_size=100000,\n",
    "    ignore_errors=True,   # salta bytes/filas con codificación mala\n",
    "    all_varchar=True,     # evita inferencia de tipos\n",
    "    delimiter=','\n",
    ")\n",
    "\n",
    "# Crea la vista 'logs' para tus queries SQL\n",
    "rel.create_view('logs')\n",
    "\n",
    "# Pruebas rápidas\n",
    "con.sql(\"SELECT * FROM logs LIMIT 5\").df()\n",
    "\n",
    "con.sql(\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT try_strptime(timestamp, '%Y-%d-%mT%H:%M:%S')    AS ts\n",
    "  FROM logs\n",
    "  UNION ALL\n",
    "  SELECT try_strptime(timestamp, '%Y-%d-%mT%H:%M')       AS ts\n",
    "  FROM logs\n",
    ")\n",
    "SELECT\n",
    "  strftime(ts, '%Y-%m') AS mes,      -- ejemplo: 2020-12\n",
    "  COUNT(*)              AS total_peticiones\n",
    "FROM t\n",
    "WHERE ts IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1;\n",
    "\n",
    "\"\"\").df()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estructura de columnas detectadas\n",
    "con.sql(\"DESCRIBE logs\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51c0ea",
   "metadata": {},
   "source": [
    "## 4) Información general del log\n",
    "\n",
    "En este apartado se realiza la consulta general del Log, de esta manera podemos analizar los datos y entender el contexto del escenario propuesto. \n",
    "\n",
    "- Para esta investigación fue necesario realizar diferentes cruces como tambien analisis de los datos, ya que al inicio se identifico cierta cantidad anomala de peticiones procedentes de diferentes sitios, dando a entender que el ataque podria tener diferentes naturalezas en comportamiento, es decir, explotaciones activas de IDOR con automatizaciones continuas sobre el API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faae657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Total de peticiones\n",
    "con.sql(\"SELECT COUNT(*) AS total_peticiones FROM logs\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867ad55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rango temporal\n",
    "con.sql(\"\"\"\n",
    "SELECT MIN(timestamp) AS fecha_inicio, MAX(timestamp) AS fecha_fin\n",
    "FROM logs\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9dc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top 20 IPs\n",
    "con.sql(\"\"\"\n",
    "SELECT source_ip, COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY source_ip\n",
    "ORDER BY total_peticiones DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fb6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Códigos HTTP (nota: columna es http_staus en el CSV)\n",
    "con.sql(\"\"\"\n",
    "SELECT http_staus AS status, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_staus\n",
    "ORDER BY cantidad DESC\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Métodos\n",
    "con.sql(\"\"\"\n",
    "SELECT http_method, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_method\n",
    "ORDER BY cantidad DESC\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usuarios-agente (Top 20)\n",
    "con.sql(\"\"\"\n",
    "SELECT user_agent, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY user_agent\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Referers (Top 20)\n",
    "con.sql(\"\"\"\n",
    "SELECT http_host, COUNT(*) AS cantidad\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY http_host\n",
    "ORDER BY cantidad DESC\n",
    "LIMIT 20\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ffa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Serie por hora (0-23)\n",
    "con.sql(\"\"\"\n",
    "SELECT strftime(timestamp, '%H') AS hora, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%/invoices/search%'\n",
    "GROUP BY hora\n",
    "ORDER BY hora\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f9db6-91b2-4c57-a2c3-190816c318da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serie por pais\n",
    "\n",
    "con.sql(\"\"\"\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN http_uri LIKE '%site_id=MeliAR%' THEN 'MeliAR - Mercado Libre Argentina'\n",
    "    WHEN http_uri LIKE '%site_id=MeliBR%' THEN 'MeliBR - Mercado Libre Brasil'\n",
    "    WHEN http_uri LIKE '%site_id=MeliMX%' THEN 'MeliMX - Mercado Libre México'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCL%' THEN 'MeliCL - Mercado Libre Chile'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCO%' THEN 'MeliCO - Mercado Libre Colombia'\n",
    "    ELSE 'Otro / No especificado'\n",
    "  END AS pais,\n",
    "  COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "WHERE http_uri LIKE '%site_id=Meli%'\n",
    "GROUP BY 1\n",
    "ORDER BY total_peticiones DESC;\n",
    "\"\"\").df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Muestras de invoice_id y tokens más consultados por esa IP\n",
    "con.sql(\"\"\"\n",
    "WITH q AS (\n",
    "  SELECT\n",
    "    regexp_extract(http_uri, 'invoice_id=([^&]+)', 1) AS invoice_id,\n",
    "    regexp_extract(http_uri, 'authtoken=([^&]+)', 1)  AS authtoken\n",
    "  FROM logs\n",
    "  WHERE http_uri LIKE '%/invoices/search%'\n",
    ")\n",
    "SELECT authtoken, COUNT(*) AS peticiones\n",
    "FROM q\n",
    "GROUP BY authtoken\n",
    "ORDER BY peticiones DESC\n",
    "\n",
    "\"\"\").df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba2c54",
   "metadata": {},
   "source": [
    "## 5) Análisis específico con Agente IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a97ccd9-33af-458e-9cce-2775c462a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Normalizador automático de columnas → crea/actualiza vista `logs` ===\n",
    "import pandas as pd\n",
    "\n",
    "schema = con.sql(\"DESCRIBE rel\").df()\n",
    "cols_lower = {c.lower(): c for c in schema[\"column_name\"].tolist()}\n",
    "\n",
    "def resolve(*candidates):\n",
    "    for c in candidates:\n",
    "        if c in cols_lower:\n",
    "            return cols_lower[c]\n",
    "    return None  # no existe\n",
    "\n",
    "# Detecta columnas reales\n",
    "c_timestamp   = resolve(\"timestamp\", \"time\", \"ts\", \"datetime\", \"date\")\n",
    "c_http_host   = resolve(\"http_host\", \"host\", \"domain\")\n",
    "c_http_uri    = resolve(\"http_uri\", \"uri\", \"path\", \"request_uri\", \"url\")\n",
    "c_http_status = resolve(\"http_status\", \"http_staus\", \"status\", \"status_code\", \"code\")\n",
    "c_http_method = resolve(\"http_method\", \"method\", \"verb\")\n",
    "c_user_agent  = resolve(\"user_agent\", \"http_user_agent\", \"ua\")\n",
    "c_client_ip   = resolve(\"client_ip\", \"source_ip\", \"ip\", \"remote_addr\", \"x_forwarded_for\")\n",
    "\n",
    "def sel(col, alias):\n",
    "    # si existe, úsalo; si no, crea NULL::VARCHAR con alias para no romper\n",
    "    return f\"{col} AS {alias}\" if col else f\"NULL::VARCHAR AS {alias}\"\n",
    "\n",
    "select_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW logs AS\n",
    "SELECT\n",
    "  {sel(c_timestamp,   'timestamp')},\n",
    "  {sel(c_http_host,   'http_host')},\n",
    "  {sel(c_http_uri,    'http_uri')},\n",
    "  {sel(c_http_status, 'http_status')},\n",
    "  {sel(c_http_method, 'http_method')},\n",
    "  {sel(c_user_agent,  'user_agent')},\n",
    "  {sel(c_client_ip,   'client_ip')}\n",
    "FROM rel\n",
    "\"\"\"\n",
    "\n",
    "con.sql(select_sql)\n",
    "\n",
    "print(\"✅ Vista `logs` creada/actualizada con columnas estandarizadas:\")\n",
    "print(con.sql(\"DESCRIBE logs\").df().to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62705b31-4652-4b21-9e6e-b176cfb5b69c",
   "metadata": {},
   "source": [
    "## IMPORTANTE ESTABLECER API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4560072-28d8-45b1-bb0d-4aec3120a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Establecimiento API KEY\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"TU_API_KEY_AQUI\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f27418-65d2-4a4c-81d8-fdd08570c0ea",
   "metadata": {},
   "source": [
    "- Para la creacion de este script, se utilizo langchain como fuente principal para el tratamiento de las LLM y poder generar las consultas a traves de OpenIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410825a-a87e-4fdd-aee8-9c270980c74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# El siguiente script permite realizar las siguientes acciones sobre la informacion cargada:\n",
    "# con análisis completo de logs + Agente IA + PDF con tablas\n",
    "#  y agente con herramienta SQL consultando el 100% del log)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ========== Configuración ==========\n",
    "CSV_FILE   = r\"TU_RUTA_LOG_AQUI\"  # Ruta confirmada\n",
    "OUTPUT_PDF = \"informe_forense_completo.pdf\"\n",
    "SAMPLE_MAX = 200   # Muestra opcional solo para contexto; el LLM puede consultar TODO con sql_agg\n",
    "\n",
    "# ========== Conexión DuckDB ==========\n",
    "try:\n",
    "    con  # si ya existe en el entorno interactivo\n",
    "except NameError:\n",
    "    con = duckdb.connect(database=\":memory:\")\n",
    "\n",
    "# ========== Helpers robustos ==========\n",
    "def table_exists(conn, name: str) -> bool:\n",
    "    \"\"\"Verifica existencia de tabla usando information_schema (sin lanzar excepción).\"\"\"\n",
    "    q = \"\"\"\n",
    "    SELECT COUNT(*) AS n\n",
    "    FROM information_schema.tables\n",
    "    WHERE table_schema IN ('main','temp') AND table_name = ?\n",
    "    \"\"\"\n",
    "    return conn.execute(q, [name]).fetchone()[0] > 0\n",
    "\n",
    "def reload_rel_from_csv_streaming(conn, csv_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Crea/actualiza la tabla 'rel' leyendo el CSV en modo 'streaming' de DuckDB.\n",
    "    No carga todo en memoria; procesa directamente desde el archivo.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise RuntimeError(f\"No existe el CSV en la ruta: {csv_path}\")\n",
    "    print(f\"📥 (Streaming) Construyendo tabla 'rel' desde: {csv_path}\")\n",
    "    conn.execute(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE rel AS\n",
    "        SELECT * FROM read_csv_auto('{csv_path}', HEADER=TRUE);\n",
    "    \"\"\")\n",
    "    print(\"✅ Tabla 'rel' creada/actualizada (streaming)\")\n",
    "\n",
    "def quoted_or_null(col: str) -> str:\n",
    "    return f'\"{col}\"' if col else \"NULL\"\n",
    "\n",
    "def df_or_empty(sql: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return con.execute(sql).df()\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def df_to_markdown_chunks(df: pd.DataFrame, chunk_size: int = 50, title: str = \"Tabla\"):\n",
    "    if df is None or df.empty:\n",
    "        return [f\"### {title}\\n_No hay datos disponibles._\\n\"]\n",
    "    chunks = []\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        sub = df.iloc[i:i+chunk_size]\n",
    "        header = f\"### {title} (filas {i+1}-{min(i+chunk_size, len(df))} de {len(df)})\"\n",
    "        chunks.append(header + \"\\n\" + sub.to_markdown(index=False) + \"\\n\")\n",
    "    return chunks\n",
    "\n",
    "def markdown_or_note(df: pd.DataFrame, title: str) -> str:\n",
    "    if df is None or df.empty:\n",
    "        return f\"### {title}\\n_No hay datos disponibles._\\n\"\n",
    "    return f\"### {title}\\n\" + df.to_markdown(index=False) + \"\\n\"\n",
    "\n",
    "# ========== Asegurar 'rel' ==========\n",
    "if not table_exists(con, \"rel\"):\n",
    "    reload_rel_from_csv_streaming(con, CSV_FILE)\n",
    "else:\n",
    "    print(\"✅ La tabla 'rel' ya existe \")\n",
    "\n",
    "# Doble verificación por si la carga falló\n",
    "if not table_exists(con, \"rel\"):\n",
    "    raise RuntimeError(\n",
    "        \"No se encontró la tabla 'rel' tras intentar cargar el CSV. \"\n",
    "        \"Verifica la ruta, permisos y que el CSV no esté vacío/corrupto.\"\n",
    "    )\n",
    "\n",
    "# ========== Vista 'logs' segura con timestamp robusto ==========\n",
    "# Detectar columnas reales presentes\n",
    "cols_df = con.execute(\"\"\"\n",
    "SELECT column_name AS name\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema IN ('main','temp') AND table_name = 'rel'\n",
    "ORDER BY ordinal_position\n",
    "\"\"\").df()\n",
    "\n",
    "raw_cols = cols_df[\"name\"].tolist()\n",
    "lowmap   = {c.lower(): c for c in raw_cols}\n",
    "def get_col(*cands):\n",
    "    for c in cands:\n",
    "        x = lowmap.get(c.lower())\n",
    "        if x: return x\n",
    "    return None\n",
    "\n",
    "c_timestamp   = get_col(\"timestamp\",\"time\",\"datetime\",\"date\")\n",
    "c_http_host   = get_col(\"http_host\",\"host\",\"server_name\",\"authority\")\n",
    "c_http_uri    = get_col(\"http_uri\",\"request_uri\",\"uri\",\"path\",\"request\")\n",
    "c_http_status = get_col(\"http_status\",\"status\",\"code\",\"http_staus\")   # tolera typo\n",
    "c_http_method = get_col(\"http_method\",\"method\",\"verb\")\n",
    "c_user_agent  = get_col(\"http_user_agent\",\"user_agent\",\"ua\")\n",
    "c_client_ip   = get_col(\"client_ip\",\"source_ip\",\"remote_addr\",\"ip\",\"src_ip\")\n",
    "\n",
    "concat_expr = \" || ' ' || \".join([f'CAST(\"{c}\" AS VARCHAR)' for c in raw_cols]) if raw_cols else \"''\"\n",
    "q_ts = quoted_or_null(c_timestamp)\n",
    "\n",
    "# Normalización de timestamp (AAAA-DD-MMTHH:MM → AAAA-MM-DDTHH:MM; + patrones alternos)\n",
    "create_logs_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW logs AS\n",
    "WITH base AS (\n",
    "  SELECT\n",
    "    COALESCE(\n",
    "      -- 1) Formato declarado: AAAA-DD-MMTHH:MM(:SS opcional)\n",
    "      try_strptime({q_ts}, '%Y-%d-%mT%H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%d-%mT%H:%M'),\n",
    "      -- 2) Reordenamos a AAAA-MM-DDTHH:MM(:SS) y parseamos estándar\n",
    "      try_strptime(\n",
    "        REGEXP_REPLACE({q_ts}, '^(\\\\d{{4}})-(\\\\d{{2}})-(\\\\d{{2}})T', '\\\\1-\\\\3-\\\\2T'),\n",
    "        '%Y-%m-%dT%H:%M:%S'\n",
    "      ),\n",
    "      try_strptime(\n",
    "        REGEXP_REPLACE({q_ts}, '^(\\\\d{{4}})-(\\\\d{{2}})-(\\\\d{{2}})T', '\\\\1-\\\\3-\\\\2T'),\n",
    "        '%Y-%m-%dT%H:%M'\n",
    "      ),\n",
    "      -- 3) Otras variantes posibles (ISO, con Z, con espacio)\n",
    "      try_strptime({q_ts}, '%Y-%m-%dT%H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%m-%d %H:%M:%S'),\n",
    "      try_strptime({q_ts}, '%Y-%m-%d'),\n",
    "      TRY_CAST({q_ts} AS TIMESTAMP)\n",
    "    ) AS timestamp,\n",
    "    {quoted_or_null(c_http_host)}   AS http_host,\n",
    "    {quoted_or_null(c_http_uri)}    AS http_uri,\n",
    "    {quoted_or_null(c_http_status)} AS http_status,\n",
    "    {quoted_or_null(c_http_method)} AS http_method,\n",
    "    {quoted_or_null(c_user_agent)}  AS user_agent,\n",
    "    {quoted_or_null(c_client_ip)}   AS client_ip,\n",
    "    REGEXP_EXTRACT({concat_expr}, '(?:\\\\d{{1,3}}\\\\.){{3}}\\\\d{{1,3}}', 0) AS derived_ip\n",
    "  FROM rel\n",
    ")\n",
    "SELECT\n",
    "  timestamp,\n",
    "  http_host,\n",
    "  http_uri,\n",
    "  http_status,\n",
    "  http_method,\n",
    "  user_agent,\n",
    "  COALESCE(client_ip, derived_ip) AS client_ip\n",
    "FROM base;\n",
    "\"\"\"\n",
    "con.execute(create_logs_sql)\n",
    "print(\"✅ Vista 'logs'\")\n",
    "\n",
    "# ========== Datos clave de investigación (100% del log) ==========\n",
    "df_timeline = df_or_empty(\"\"\"\n",
    "SELECT CAST(timestamp AS DATE) AS fecha, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "WHERE timestamp IS NOT NULL\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\")\n",
    "\n",
    "df_status = df_or_empty(\"\"\"\n",
    "SELECT http_status AS status, COUNT(*) AS total\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "\"\"\")\n",
    "\n",
    "df_endpoints = df_or_empty(\"\"\"\n",
    "SELECT http_uri AS endpoint, COUNT(*) AS peticiones\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY peticiones DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "df_ua = df_or_empty(\"\"\"\n",
    "SELECT user_agent, COUNT(*) AS total\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "LIMIT 25\n",
    "\"\"\")\n",
    "\n",
    "# IPs (evidencia resumida)\n",
    "df_ip = df_or_empty(\"\"\"\n",
    "SELECT client_ip AS ip, COUNT(*) AS total\n",
    "FROM logs\n",
    "WHERE client_ip IS NOT NULL AND client_ip <> ''\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "# TODAS las IPs (para CSV; no se insertará en el PDF)\n",
    "df_ip_full = df_or_empty(\"\"\"\n",
    "SELECT client_ip AS ip, COUNT(*) AS total\n",
    "FROM logs\n",
    "WHERE client_ip IS NOT NULL AND client_ip <> ''\n",
    "GROUP BY 1\n",
    "ORDER BY total DESC\n",
    "\"\"\")\n",
    "df_ip_full.to_csv(\"ips_conteo_completo.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"💾 Exportado CSV: ips_conteo_completo.csv\")\n",
    "\n",
    "# Heurística IDOR\n",
    "df_idor = df_or_empty(\"\"\"\n",
    "SELECT http_uri AS endpoint, COUNT(*) AS hits\n",
    "FROM logs\n",
    "WHERE http_uri ~ '(\\\\?|&)(id|uid|user|invoice|order|doc|account|acc)='\n",
    "   OR http_uri ~ '/\\\\d+'\n",
    "GROUP BY 1\n",
    "ORDER BY hits DESC\n",
    "LIMIT 500\n",
    "\"\"\")\n",
    "\n",
    "# Authtoken más consultados (global) por /invoices/search\n",
    "df_authtoken_global = con.execute(\"\"\"\n",
    "WITH q AS (\n",
    "  SELECT\n",
    "    regexp_extract(http_uri, 'invoice_id=([^&]+)', 1) AS invoice_id,\n",
    "    regexp_extract(http_uri, 'authtoken=([^&]+)', 1)  AS authtoken\n",
    "  FROM logs\n",
    "  WHERE http_uri LIKE '%/invoices/search%'\n",
    ")\n",
    "SELECT authtoken, COUNT(*) AS peticiones\n",
    "FROM q\n",
    "GROUP BY authtoken\n",
    "ORDER BY peticiones DESC\n",
    "\"\"\").df()\n",
    "df_authtoken_global.to_csv(\"authtoken_global.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"💾 Exportado CSV: authtoken_global.csv\")\n",
    "\n",
    "# Volumen de peticiones por país (site_id)\n",
    "df_por_pais = df_or_empty(\"\"\"\n",
    "SELECT\n",
    "  CASE\n",
    "    WHEN http_uri LIKE '%site_id=MeliAR%' THEN 'MeliAR - Mercado Libre Argentina'\n",
    "    WHEN http_uri LIKE '%site_id=MeliBR%' THEN 'MeliBR - Mercado Libre Brasil'\n",
    "    WHEN http_uri LIKE '%site_id=MeliMX%' THEN 'MeliMX - Mercado Libre México'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCL%' THEN 'MeliCL - Mercado Libre Chile'\n",
    "    WHEN http_uri LIKE '%site_id=MeliCO%' THEN 'MeliCO - Mercado Libre Colombia'\n",
    "    ELSE 'Otro / No identificado'\n",
    "  END AS site_id,\n",
    "  COUNT(*) AS total_peticiones\n",
    "FROM logs\n",
    "GROUP BY 1\n",
    "ORDER BY total_peticiones DESC\n",
    "\"\"\")\n",
    "df_por_pais.to_csv(\"site_id_resumen.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"💾 Exportado CSV: site_id_resumen.csv\")\n",
    "\n",
    "# ========== Evidencia Markdown para IA ==========\n",
    "evidencia_md = \"\\n\".join([\n",
    "    \"*(Todas las tablas de esta sección se calcularon sobre el **100%** del log)*\\n\",\n",
    "    markdown_or_note(df_timeline,   \"Timeline diario\"),\n",
    "    markdown_or_note(df_por_pais,   \"Volumen de peticiones por país (site_id)\"),\n",
    "    markdown_or_note(df_status,     \"Distribución de códigos de estado\"),\n",
    "    markdown_or_note(df_endpoints,  \"Endpoints más solicitados\"),\n",
    "    markdown_or_note(df_ua,         \"User-Agents más frecuentes\"),\n",
    "    markdown_or_note(df_ip,         \"Direcciones IP con mayor volumen (top 100)\"),\n",
    "    markdown_or_note(df_authtoken_global.head(50), \"Authtoken más consultados — /invoices/search (top 50)\"),\n",
    "    markdown_or_note(df_idor,       \"Patrones posibles de IDOR (heurística)\"),\n",
    "])\n",
    "\n",
    "# Muestra pequeña opcional (el LLM PUEDE IGNORARLA; tiene acceso a sql_agg para 100%)\n",
    "df_logs_full = df_or_empty(\"SELECT * FROM logs\")\n",
    "if df_logs_full.empty:\n",
    "    sample_md = \"_El conjunto de logs está vacío._\"\n",
    "else:\n",
    "    sample_n = min(len(df_logs_full), SAMPLE_MAX)\n",
    "    df_sample = df_logs_full.sample(sample_n, random_state=42) if len(df_logs_full) > sample_n else df_logs_full\n",
    "    sample_md = df_sample.head(SAMPLE_MAX).to_markdown(index=False)\n",
    "\n",
    "# ========== Agente IA (LangChain) — con herramienta SQL sobre el 100% ==========\n",
    "respuesta_ia = None\n",
    "ia_error = None\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.agents import initialize_agent, AgentType\n",
    "    from langchain.tools import tool\n",
    "\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    if not OPENAI_API_KEY:\n",
    "        raise RuntimeError(\n",
    "            \"No se encontró OPENAI_API_KEY en el entorno. \"\n",
    "            \"Configura la variable de entorno antes de ejecutar este script.\"\n",
    "        )\n",
    "\n",
    "    # Herramienta segura para consultas SELECT sobre TODO el log.\n",
    "    @tool(\"sql_agg\", return_direct=False)\n",
    "    def sql_agg(sql: str) -> str:\n",
    "        \"\"\"\n",
    "        Ejecuta consultas SQL **solo-lectura (SELECT)** sobre la base completa (100% del log).\n",
    "        Devuelve el resultado en Markdown. Limita a 2000 filas para no exceder el tamaño.\n",
    "        Seguridad:\n",
    "          - Rechaza consultas que no comiencen por 'SELECT' (ignorando espacios/lineas).\n",
    "          - Rechaza palabras peligrosas: CREATE/INSERT/UPDATE/DELETE/ALTER/DROP/REPLACE/ATTACH/COPY.\n",
    "        \"\"\"\n",
    "        if not isinstance(sql, str) or not sql.strip():\n",
    "            return \"⚠️ sql_agg: Debes enviar una consulta SELECT no vacía.\"\n",
    "        sql_clean = sql.strip().lstrip(\"(\\n\\t \").upper()\n",
    "        if not sql_clean.startswith(\"SELECT\"):\n",
    "            return \"⛔ sql_agg: Solo se permiten consultas SELECT.\"\n",
    "        forbidden = (\"CREATE\",\"INSERT\",\"UPDATE\",\"DELETE\",\"ALTER\",\"DROP\",\"REPLACE\",\"ATTACH\",\"COPY\",\"PRAGMA\",\"IMPORT\",\"EXPORT\",\"LOAD\")\n",
    "        if any(w in sql_clean for w in forbidden):\n",
    "            return \"⛔ sql_agg: Consulta rechazada por contener palabras no permitidas.\"\n",
    "        try:\n",
    "            df = con.execute(sql).df()\n",
    "            if df.empty:\n",
    "                return \"_(sin filas)_\"\n",
    "            # Limitar filas para respuesta\n",
    "            if len(df) > 2000:\n",
    "                df = df.head(2000)\n",
    "                note = \"\\n_(resultado truncado a 2000 filas)_\\n\"\n",
    "            else:\n",
    "                note = \"\"\n",
    "            return df.to_markdown(index=False) + note\n",
    "        except Exception as e:\n",
    "            return f\" Error ejecutando SQL: {e}\"\n",
    "\n",
    "    @tool(\"get_aggregates_md\", return_direct=False)\n",
    "    def get_aggregates_md(_: str = \"\") -> str:\n",
    "        \"\"\"Devuelve las tablas agregadas (100% del log) en Markdown.\"\"\"\n",
    "        return evidencia_md\n",
    "\n",
    "    @tool(\"get_sample_md\", return_direct=False)\n",
    "    def get_sample_md(_: str = \"\") -> str:\n",
    "        \"\"\"Devuelve una muestra representativa pequeña del log en Markdown (opcional).\"\"\"\n",
    "        return sample_md\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    tools = [sql_agg, get_aggregates_md, get_sample_md]\n",
    "\n",
    "    try:\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=False\n",
    "        )\n",
    "    except TypeError:\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Eres un analista forense experto. Todo el **contexto agregado** proviene del **100% del log**.\n",
    "Si necesitas verificar cualquier detalle, puedes usar la herramienta **sql_agg** para ejecutar\n",
    "consultas SQL *SELECT* directamente sobre el conjunto de datos completo (100% del log).\n",
    "\n",
    "## Evidencia agregada (100% del log; tablas)\n",
    "{evidencia_md}\n",
    "\n",
    "## Muestra opcional (solo referencia rápida; puedes ignorarla si usas sql_agg)\n",
    "{sample_md}\n",
    "\n",
    "Con base en TODO el log (usa preferentemente los agregados del 100% y, si hace falta,\n",
    "consulta con sql_agg), elabora un **informe forense completo en Markdown** con la siguiente estructura y formato:\n",
    "\n",
    "## Resumen ejecutivo\n",
    "- Contexto del incidente, alcance e impacto potencial, partiendo del LOG completo. Indica la vulnerabilidad predominante si la evidencias.\n",
    "\n",
    "## Narrativa del incidente\n",
    "- Línea de tiempo de eventos relevantes y fases del ataque (usa timeline y/o agrégalo con sql_agg si requieres granularidad).\n",
    "\n",
    "## Indicadores de compromiso (IoC)\n",
    "- Direcciones IP, endpoints, user-agents y comportamientos anómalos. Presenta **tablas** cuando corresponda.\n",
    "\n",
    "## Posibles patrones IDOR\n",
    "- Evidencias de manipulación de parámetros y accesos indebidos (usa **tablas** y justifica por qué podrían ser IDOR).\n",
    "\n",
    "## Estadísticas clave del incidente\n",
    "- **Tablas Markdown** con la distribución de status, endpoints más atacados, volumen por IP, site_id/país, etc.\n",
    "\n",
    "## Análisis de User-Agents\n",
    "- Destaca bots o automatizaciones (wget, curl, python, scrapy, spider, bot) con **tablas**.\n",
    "\n",
    "## Recomendaciones\n",
    "- Acciones inmediatas y mitigaciones a mediano y largo plazo, detalladas.\n",
    "\n",
    "### Reglas estrictas de formato\n",
    "- Usa **títulos H2/H3** consistentes.\n",
    "- Donde presentes datos, usa **tablas Markdown** (| col1 | col2 | ... |).\n",
    "- **NO RESUMAS** filas de tablas: si una tabla es larga, divídela en bloques de 50 filas, **sin omitir filas**.\n",
    "- No incluyas trazas ni llamadas a herramientas en la respuesta (solo resultados).\n",
    "\"\"\"\n",
    "    respuesta_ia = agent.run(prompt)\n",
    "    print(\"Respuesta IA (vista previa):\\n\", (respuesta_ia or \"\")[:1500])\n",
    "\n",
    "except Exception as e:\n",
    "    ia_error = repr(e)\n",
    "    print(\"ℹ️ No se pudo ejecutar el agente IA:\", ia_error)\n",
    "    partes = [\n",
    "        \"# Informe Forense (sin IA)\",\n",
    "        \"## Resumen\",\n",
    "        \"No se pudo ejecutar el modelo. Se incluyen agregados para referencia (100% del log).\",\n",
    "        evidencia_md\n",
    "    ]\n",
    "    respuesta_ia = \"\\n\\n\".join(partes)\n",
    "\n",
    "# ========== Sección fija adicional en el PDF ==========\n",
    "# 1) Authtoken completos (en bloques para no truncar)\n",
    "authtoken_chunks = df_to_markdown_chunks(\n",
    "    df_authtoken_global,\n",
    "    chunk_size=50,\n",
    "    title=\"Authtoken más consultados — /invoices/search (completo)\"\n",
    ")\n",
    "seccion_authtoken = \"\\n\".join(authtoken_chunks)\n",
    "\n",
    "# 2) Volumen de peticiones por país (site_id) — sección solicitada\n",
    "site_id_chunks = df_to_markdown_chunks(\n",
    "    df_por_pais,\n",
    "    chunk_size=50,\n",
    "    title=\"Volumen de peticiones por país (site_id)\"\n",
    ")\n",
    "seccion_site_id = \"\\n\".join(site_id_chunks)\n",
    "\n",
    "# Unir secciones fijas al informe final\n",
    "respuesta_ia = (\n",
    "    (respuesta_ia or \"# Informe Forense\")\n",
    "    + \"\\n\\n## Sección fija — Volumen de peticiones por país (site_id)\\n\"\n",
    "    + seccion_site_id\n",
    "    + \"\\n\\n> Archivo complementario: `site_id_resumen.csv`\\n\"\n",
    "    + \"\\n\\n## Sección fija — Authtoken más consultados\\n\"\n",
    "    + seccion_authtoken\n",
    "    + \"\\n\\n> Archivo complementario: `authtoken_global.csv`\"\n",
    ")\n",
    "\n",
    "# ========== Conversión Markdown -> PDF (ReportLab con auto-wrap de celdas) ==========\n",
    "try:\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib import colors\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, ListFlowable, ListItem\n",
    "    from reportlab.lib.units import cm\n",
    "\n",
    "    styles = getSampleStyleSheet()\n",
    "    STY_H1 = ParagraphStyle(\"H1\", parent=styles[\"Heading1\"], fontSize=16, leading=20, spaceAfter=10)\n",
    "    STY_H2 = ParagraphStyle(\"H2\", parent=styles[\"Heading2\"], fontSize=14, leading=18, spaceAfter=8)\n",
    "    STY_H3 = ParagraphStyle(\"H3\", parent=styles[\"Heading3\"], fontSize=12, leading=16, spaceAfter=6)\n",
    "    STY_BODY = ParagraphStyle(\"BODY\", parent=styles[\"BodyText\"], fontSize=10.0, leading=13.5, spaceAfter=6)\n",
    "\n",
    "    def parse_md_tables(md_text: str):\n",
    "        \"\"\"Devuelve lista de bloques: {'type': 'table'|'heading'|'list'|'p', ...}\"\"\"\n",
    "        lines = md_text.splitlines()\n",
    "        blocks, buf_para = [], []\n",
    "\n",
    "        def flush_para():\n",
    "            if buf_para:\n",
    "                blocks.append({\"type\":\"p\", \"text\":\"\\n\".join(buf_para).strip()})\n",
    "                buf_para.clear()\n",
    "\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].rstrip()\n",
    "\n",
    "            # Encabezados\n",
    "            if re.match(r\"^#{1,6}\\s+\", line):\n",
    "                flush_para()\n",
    "                level = len(line) - len(line.lstrip(\"#\"))\n",
    "                text = line[level:].strip()\n",
    "                blocks.append({\"type\":\"heading\", \"level\":level, \"text\":text})\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # Listas\n",
    "            if re.match(r\"^\\s*[-*]\\s+\", line):\n",
    "                flush_para()\n",
    "                items = []\n",
    "                while i < len(lines) and re.match(r\"^\\s*[-*]\\s+\", lines[i]):\n",
    "                    items.append(re.sub(r\"^\\s*[-*]\\s+\", \"\", lines[i]).strip())\n",
    "                    i += 1\n",
    "                blocks.append({\"type\":\"list\", \"items\":items})\n",
    "                continue\n",
    "\n",
    "            # Tablas Markdown\n",
    "            if '|' in line:\n",
    "                if i+1 < len(lines) and re.match(r'^\\s*\\|?[:\\-| ]+\\|?\\s*$', lines[i+1]):\n",
    "                    tbl_lines = [line, lines[i+1]]\n",
    "                    i += 2\n",
    "                    while i < len(lines) and '|' in lines[i] and lines[i].strip():\n",
    "                        tbl_lines.append(lines[i])\n",
    "                        i += 1\n",
    "                    rows = []\n",
    "                    for idx, tline in enumerate(tbl_lines):\n",
    "                        parts = [c.strip() for c in tline.strip().strip('|').split('|')]\n",
    "                        if idx == 1:\n",
    "                            continue  # separador\n",
    "                        rows.append(parts)\n",
    "                    if rows:\n",
    "                        flush_para()\n",
    "                        blocks.append({\"type\":\"table\", \"rows\":rows})\n",
    "                    continue\n",
    "\n",
    "            # Párrafos\n",
    "            if line.strip() == \"\":\n",
    "                flush_para()\n",
    "            else:\n",
    "                buf_para.append(line)\n",
    "            i += 1\n",
    "\n",
    "        flush_para()\n",
    "        return blocks\n",
    "\n",
    "    # ====== AJUSTE DE TABLAS: ancho máximo + auto-wrap en celdas ======\n",
    "    from reportlab.platypus import Table, TableStyle, Paragraph\n",
    "    from reportlab.lib.units import cm\n",
    "\n",
    "    def md_to_story(md_text: str):\n",
    "        blocks = parse_md_tables(md_text or \"\")\n",
    "        story = []\n",
    "        # Portada\n",
    "        story.append(Paragraph(\"Informe Forense Automatizado\", STY_H1))\n",
    "        story.append(Paragraph(datetime.now().strftime(\"%Y-%m-%d %H:%M\"), STY_BODY))\n",
    "        story.append(Spacer(1, 0.5*cm))\n",
    "\n",
    "        for b in blocks:\n",
    "            if b[\"type\"] == \"heading\":\n",
    "                sty = STY_H1 if b[\"level\"] <= 1 else STY_H2 if b[\"level\"] == 2 else STY_H3\n",
    "                story.append(Paragraph(b[\"text\"], sty))\n",
    "\n",
    "            elif b[\"type\"] == \"list\":\n",
    "                items = [ListItem(Paragraph(it.replace(\"\\n\",\"<br/>\"), STY_BODY)) for it in b[\"items\"]]\n",
    "                story.append(ListFlowable(items, bulletType=\"bullet\"))\n",
    "\n",
    "            elif b[\"type\"] == \"table\":\n",
    "                data = b[\"rows\"]\n",
    "\n",
    "                # Limitar ancho máximo de tabla: A4(21cm) - márgenes (2+2) = 17cm\n",
    "                max_table_width = 17 * cm\n",
    "                num_cols = max(len(data[0]), 1)\n",
    "                col_width = max_table_width / num_cols\n",
    "\n",
    "                # Envolver texto en celdas largas con saltos de línea\n",
    "                wrapped_data = []\n",
    "                for row in data:\n",
    "                    wrapped_row = []\n",
    "                    for cell in row:\n",
    "                        text = str(cell)\n",
    "                        # Si la celda es muy larga, inserta saltos cada ~60 chars\n",
    "                        if len(text) > 60:\n",
    "                            text = \"<br/>\".join([text[i:i+60] for i in range(0, len(text), 60)])\n",
    "                        wrapped_row.append(Paragraph(text, STY_BODY))\n",
    "                    wrapped_data.append(wrapped_row)\n",
    "\n",
    "                t = Table(wrapped_data, colWidths=[col_width]*num_cols, repeatRows=1)\n",
    "                t.setStyle(TableStyle([\n",
    "                    ('FONTNAME',(0,0),(-1,-1),'Helvetica'),\n",
    "                    ('FONTSIZE',(0,0),(-1,-1),8.5),\n",
    "                    ('BACKGROUND',(0,0),(-1,0),colors.HexColor(\"#F0F2F5\")),\n",
    "                    ('TEXTCOLOR',(0,0),(-1,0),colors.black),\n",
    "                    ('GRID',(0,0),(-1,-1),0.3,colors.HexColor(\"#B8C1CC\")),\n",
    "                    ('ALIGN',(0,0),(-1,0),'CENTER'),\n",
    "                    ('ALIGN',(0,1),(-1,-1),'LEFT'),\n",
    "                    ('VALIGN',(0,0),(-1,-1),'TOP'),\n",
    "                    ('ROWBACKGROUNDS',(0,1),(-1,-1),[colors.white, colors.HexColor(\"#FCFEFF\")]),\n",
    "                    ('TOPPADDING',(0,0),(-1,-1),3),\n",
    "                    ('BOTTOMPADDING',(0,0),(-1,-1),3),\n",
    "                ]))\n",
    "                story.append(t)\n",
    "\n",
    "            else:  # párrafo\n",
    "                text = b[\"text\"].replace(\"&\", \"&amp;\").replace(\"<\",\"&lt;\").replace(\">\",\"&gt;\")\n",
    "                story.append(Paragraph(text.replace(\"\\n\",\"<br/>\"), STY_BODY))\n",
    "\n",
    "            story.append(Spacer(1, 0.25*cm))\n",
    "        return story\n",
    "\n",
    "    from reportlab.platypus import SimpleDocTemplate\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "\n",
    "    doc = SimpleDocTemplate(\n",
    "        OUTPUT_PDF, pagesize=A4,\n",
    "        topMargin=1.5*cm, bottomMargin=1.5*cm, leftMargin=2*cm, rightMargin=2*cm\n",
    "    )\n",
    "    story = md_to_story(respuesta_ia or \"# Informe vacío\")\n",
    "    doc.build(story)\n",
    "    print(f\"✅ PDF generado: {os.path.abspath(OUTPUT_PDF)}\")\n",
    "    print(\"CSVs generados: ips_conteo_completo.csv, authtoken_global.csv, site_id_resumen.csv\")\n",
    "    if ia_error:\n",
    "        print(\" Nota: el PDF se generó, pero la IA falló y se usó contenido de respaldo.\")\n",
    "\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"Falta una librería para exportar PDF:\", e)\n",
    "    print(\"Instala con:  pip install reportlab\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75fb98d-dc9d-4838-9bd7-9c549f98a3f5",
   "metadata": {},
   "source": [
    "## 6) ALMACENADOR DE MD PARA USO DE CHAT INTERACTIVO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541a692-de53-407a-af19-ceeaf7b04a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# Guardar el informe en Markdown, es necesario este paso para poder usar el el chat interactivo, este se genera a traves del uso de la anterior celda.\n",
    "# - Usa la salida real del agente (respuesta_ia) si existe\n",
    "# - Si no, guarda un respaldo con la evidencia agregada\n",
    "# - Normaliza saltos de línea, asegura UTF-8 y crea una versión con timestamp\n",
    "# ================================================\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "def _safe_text(s) -> str:\n",
    "    \"\"\"Convierte a str, normaliza saltos de línea y asegura fin de archivo con newline.\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    # normaliza CRLF/CR a LF\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # quita null bytes por si vinieran de alguna conversión extraña\n",
    "    s = s.replace(\"\\x00\", \"\")\n",
    "    # asegura 1 salto de línea final\n",
    "    if not s.endswith(\"\\n\"):\n",
    "        s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "def _build_fallback_md() -> str:\n",
    "    partes = [\n",
    "        \"# Informe Forense (respaldo)\\n\",\n",
    "        f\"_Generado automáticamente: {datetime.now().strftime('%Y-%m-%d %H:%M')}_\\n\",\n",
    "        \"## Nota\\nNo se obtuvo salida del modelo IA. Se incluyen agregados para referencia.\\n\",\n",
    "    ]\n",
    "    ev = globals().get(\"evidencia_md\")\n",
    "    if isinstance(ev, str) and ev.strip():\n",
    "        partes.append(ev)\n",
    "    else:\n",
    "        partes.append(\"_No hay evidencia agregada disponible._\")\n",
    "    return \"\\n\\n\".join(partes)\n",
    "\n",
    "# 1) Determinar el contenido a guardar\n",
    "contenido_md = None\n",
    "resp = globals().get(\"respuesta_ia\")\n",
    "if isinstance(resp, str) and resp.strip():\n",
    "    contenido_md = resp\n",
    "else:\n",
    "    contenido_md = _build_fallback_md()\n",
    "\n",
    "contenido_md = _safe_text(contenido_md)\n",
    "\n",
    "# 2) Construir nombre de salida\n",
    "#    - base: informe_forense_completo.md (compatible con tu flujo)\n",
    "#    - copia con timestamp para trazabilidad\n",
    "base_name = \"informe_forense_completo\"\n",
    "OUTPUT_MD = f\"{base_name}.md\"\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTPUT_MD_TS = f\"{base_name}_{ts}.md\"\n",
    "\n",
    "# 3) (Opcional) Inyectar metadatos mínimos al inicio si no existen encabezados\n",
    "if not contenido_md.lstrip().startswith(\"# \"):\n",
    "    meta = [\n",
    "        f\"# Informe Forense Automatizado\",\n",
    "        f\"_Generado: {datetime.now().strftime('%Y-%m-%d %H:%M')}_\",\n",
    "        \"\",\n",
    "    ]\n",
    "    contenido_md = _safe_text(\"\\n\".join(meta) + contenido_md)\n",
    "\n",
    "# 4) Escribir a disco (UTF-8) en dos archivos: el “canonical” y la copia con timestamp\n",
    "def _write_utf8(path: str, text: str):\n",
    "    # Usa io.open para asegurar comportamiento consistente en distintos entornos\n",
    "    with io.open(path, \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "try:\n",
    "    _write_utf8(OUTPUT_MD, contenido_md)\n",
    "    _write_utf8(OUTPUT_MD_TS, contenido_md)\n",
    "\n",
    "    # 5) Mensajes de confirmación útiles\n",
    "    abs_main = os.path.abspath(OUTPUT_MD)\n",
    "    abs_ts   = os.path.abspath(OUTPUT_MD_TS)\n",
    "\n",
    "    size_main = os.path.getsize(OUTPUT_MD)\n",
    "    size_ts   = os.path.getsize(OUTPUT_MD_TS)\n",
    "\n",
    "    print(\"✅ Informe Markdown guardado correctamente.\")\n",
    "    print(f\"   • Principal : {abs_main}  ({size_main:,} bytes)\")\n",
    "    print(f\"   • Timestamp : {abs_ts}  ({size_ts:,} bytes)\")\n",
    "\n",
    "    # 6) Consejos de integración con tu flujo actual\n",
    "    #    - Si usas el chat forense interactivo, éste buscará 'informe_forense_completo.md' por defecto.\n",
    "    #    - La copia con timestamp te permite versionar rápidamente entregables.\n",
    "\n",
    "except Exception as e:\n",
    "    # Falla segura con explicación\n",
    "    print(\"⛔ Error al guardar el informe en Markdown:\", e, file=sys.stderr)\n",
    "    print(\"   Verifica permisos de escritura y rutas.\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75318908-c0fa-43ce-8a50-a0421009c109",
   "metadata": {},
   "source": [
    "## 7) CHAT INTERACTIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92acc760-f3cf-4930-9807-a6a8a191adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Chat\n",
    "# - Carga robusta del informe Markdown\n",
    "# - QA con LLM usando el informe como contexto (recorte inteligente)\n",
    "# - Búsqueda de fragmentos relevantes en el informe\n",
    "# - Presets de preguntas forenses frecuentes\n",
    "# ================================================================\n",
    "\n",
    "import os, re, textwrap, pathlib\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# ---------- 1) Localización robusta del informe ----------\n",
    "def _find_latest_report(base_name=\"informe_forense_completo\", exts=(\".md\",)):\n",
    "    \"\"\"Devuelve la ruta del informe más reciente.\"\"\"\n",
    "    candidates = []\n",
    "    cwd = pathlib.Path(\".\")\n",
    "    for ext in exts:\n",
    "        candidates += list(cwd.glob(f\"{base_name}{ext}\"))\n",
    "        candidates += list(cwd.glob(f\"{base_name}_*.{ext.strip('.')}\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    return str(candidates[0])\n",
    "\n",
    "if \"out_md\" in globals() and isinstance(out_md, str) and os.path.exists(out_md):\n",
    "    INFORME_PATH = out_md\n",
    "else:\n",
    "    INFORME_PATH = _find_latest_report() or \"informe_forense_completo.md\"\n",
    "\n",
    "if not os.path.exists(INFORME_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"No encontré el informe: {INFORME_PATH}\\n\"\n",
    "        \"Asegúrate de haber generado el Markdown o ajusta INFORME_PATH.\"\n",
    "    )\n",
    "\n",
    "with open(INFORME_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    INFORME_MD = f.read()\n",
    "\n",
    "# ---------- 2) Utilidades de contexto ----------\n",
    "def _trim_context_sections(md: str, max_chars: int = 120_000) -> str:\n",
    "    \"\"\"Recorte inteligente manteniendo las secciones más relevantes.\"\"\"\n",
    "    md = md.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    if len(md) <= max_chars:\n",
    "        return md\n",
    "\n",
    "    parts = re.split(r'(?m)^#{2,3}\\s+', md)\n",
    "    headers = re.findall(r'(?m)^#{2,3}\\s+(.+)$', md)\n",
    "    sections = []\n",
    "    lead = parts[0]\n",
    "    for h, body in zip(headers, parts[1:]):\n",
    "        sections.append((h.strip(), body))\n",
    "\n",
    "    signals = re.compile(\n",
    "        r'\\b(401|403|404|500|SQL|IDOR|token|authtoken|invoice|/invoices|login|auth|csrf|xss|timeout|error|bot|curl|wget|python|spider|scrapy|endpoint|site_id|ip|user-agent)\\b',\n",
    "        re.I\n",
    "    )\n",
    "    scored = []\n",
    "    for h, body in sections:\n",
    "        score = len(signals.findall(body)) + len(signals.findall(h))\n",
    "        scored.append((score, h, body))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    keep = []\n",
    "    total = len(lead)\n",
    "    keep.append(lead)\n",
    "    i = 0\n",
    "    while i < len(scored) and total < max_chars * 0.85:\n",
    "        score, h, body = scored[i]\n",
    "        chunk = f\"\\n\\n## {h}\\n{body}\"\n",
    "        total += len(chunk)\n",
    "        keep.append(chunk)\n",
    "        i += 1\n",
    "\n",
    "    tail = md[-min(len(md)//6, int(max_chars*0.15)):]\n",
    "    keep.append(\"\\n\\n...[contexto recortado]...\\n\\n\")\n",
    "    keep.append(tail)\n",
    "    return \"\".join(keep)\n",
    "\n",
    "CTX_FULL = INFORME_MD\n",
    "CTX = _trim_context_sections(INFORME_MD)\n",
    "\n",
    "def _context_snippets(query: str, md: str, window=220, max_hits=6) -> List[str]:\n",
    "    \"\"\"Extrae fragmentos relevantes del informe.\"\"\"\n",
    "    if not query or not query.strip():\n",
    "        return []\n",
    "    toks = [re.escape(t) for t in re.findall(r'\\w{3,}', query.lower())]\n",
    "    if not toks:\n",
    "        return []\n",
    "    pat = re.compile(\"|\".join(toks), re.I)\n",
    "    hits = []\n",
    "    for m in pat.finditer(md):\n",
    "        start = max(0, m.start() - window)\n",
    "        end = min(len(md), m.end() + window)\n",
    "        frag = md[start:end].replace(\"\\n\", \" \")\n",
    "        hits.append(\"… \" + frag + \" …\")\n",
    "        if len(hits) >= max_hits:\n",
    "            break\n",
    "    return hits\n",
    "\n",
    "# ---------- 3) LLM (LangChain + OpenAI) ----------\n",
    "LLM_READY = True\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "except Exception as e:\n",
    "    LLM_READY = False\n",
    "    _IMPORT_ERR = repr(e)\n",
    "\n",
    "if LLM_READY:\n",
    "    try:\n",
    "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    except Exception as e:\n",
    "        LLM_READY = False\n",
    "        _INIT_ERR = repr(e)\n",
    "\n",
    "# ---------- 4) Diálogo forense ----------\n",
    "def dialogo_forense(pregunta: str, incluir_snippets: bool = True) -> str:\n",
    "    \"\"\"Pregunta a la IA usando el informe forense como contexto.\"\"\"\n",
    "    if not pregunta or not pregunta.strip():\n",
    "        return \"Por favor escribe una pregunta.\"\n",
    "\n",
    "    snippets = _context_snippets(pregunta, CTX_FULL) if incluir_snippets else []\n",
    "\n",
    "    if not LLM_READY:\n",
    "        detalles = []\n",
    "        if \"_IMPORT_ERR\" in globals(): detalles.append(f\"ImportError: {_IMPORT_ERR}\")\n",
    "        if \"_INIT_ERR\" in globals():   detalles.append(f\"InitError: {_INIT_ERR}\")\n",
    "        extra = (\"\\n\\nDetalles: \" + \" | \".join(detalles)) if detalles else \"\"\n",
    "        base = (\n",
    "            \"⚠️ El modelo de IA no está disponible. \"\n",
    "            \"Verifica tu API Key y que `langchain-openai` esté instalado.\"\n",
    "            + extra\n",
    "        )\n",
    "        if snippets:\n",
    "            base += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return base\n",
    "\n",
    "    instruct = (\n",
    "        \"Eres un analista forense experto. Responde en español, claro y accionable. \"\n",
    "        \"Si algo no está en el contexto, dilo explícitamente y sugiere qué dato falta.\"\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", instruct),\n",
    "        (\"system\", f\"Contexto del informe (Markdown recortado):\\n{CTX}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ])\n",
    "    try:\n",
    "        msg = prompt.format_messages(question=pregunta)\n",
    "        ans = llm.invoke(msg)\n",
    "        out = ans.content if hasattr(ans, \"content\") else str(ans)\n",
    "\n",
    "        if snippets:\n",
    "            out += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        base = (\n",
    "            \"⚠️ No pude obtener respuesta del modelo.\\n\\n\"\n",
    "            f\"Error: {e}\\n\\n\"\n",
    "            \"Sugerencias: prueba con una pregunta más corta o revisa los límites de tokens.\"\n",
    "        )\n",
    "        if snippets:\n",
    "            base += \"\\n\\n### Fragmentos relevantes\\n\" + \"\\n\\n\".join([f\"> {s}\" for s in snippets])\n",
    "        return base\n",
    "\n",
    "# ---------- 5) UI con ipywidgets ----------\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "PRESETS = [\n",
    "    \"Resumen del incidente\",\n",
    "    \"Principales indicadores de compromiso\",\n",
    "    \"Patrones IDOR detectados\",\n",
    "    \"Recomendaciones inmediatas\",\n",
    "    \"Análisis de User-Agents sospechosos\",\n",
    "    \"Fases del ataque en la línea de tiempo\",\n",
    "]\n",
    "\n",
    "inp_q = widgets.Textarea(\n",
    "    placeholder='Haz una pregunta sobre la investigación forense…',\n",
    "    description='Pregunta:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "chk_snip = widgets.Checkbox(value=True, description=\"Incluir fragmentos del informe\")\n",
    "ddl_preset = widgets.Dropdown(options=PRESETS, description=\"Preguntas rápidas:\")\n",
    "btn = widgets.Button(description=\"Preguntar\", button_style='primary', icon=\"search\")\n",
    "out = widgets.Output(layout={'border': '1px solid #ddd'})\n",
    "\n",
    "def _on_preset_change(change):\n",
    "    if change['name'] == 'value':\n",
    "        inp_q.value = f\"Explícame: {change['new']}\"\n",
    "\n",
    "ddl_preset.observe(_on_preset_change, names='value')\n",
    "\n",
    "def _on_click(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        print(\"🧠 Respuesta:\")\n",
    "        resp = dialogo_forense(inp_q.value, incluir_snippets=chk_snip.value)\n",
    "        display(Markdown(resp))\n",
    "\n",
    "btn.on_click(_on_click)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    inp_q,\n",
    "    chk_snip,\n",
    "    ddl_preset,\n",
    "    btn,\n",
    "    widgets.HTML(\"<b>💬 Respuesta:</b>\"),\n",
    "    out\n",
    "]))\n",
    "\n",
    "print(f\"📄 Informe cargado: {os.path.abspath(INFORME_PATH)}\")\n",
    "print(f\"🪪 LLM listo: {LLM_READY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01843f55-a158-46de-887e-0def855b4f2d",
   "metadata": {},
   "source": [
    "## GRACIAS POR LA ATENCION PRESTADA!!!\n",
    "\n",
    "- **NOTA:El notebook, esta organizado para que sea ejecutado celda por celda, de esta manera se garantiza la linealidad del funcionamiento del codigo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0b757-f37b-4f6e-8f01-699f6a4ae60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
